{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2714e0ed"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import shutup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.linear_model import PoissonRegressor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:52:20.854757500Z",
     "start_time": "2024-01-04T16:52:19.178962500Z"
    }
   },
   "id": "e051ae22ea092cdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load train data\n",
    "\n",
    "- iterate through train data folder \n",
    "- feature engineer the **country** and **league** from parent folder name\n",
    "- join the loaded csv files by rows\n",
    "- removed quotation marks that would in some rows merge two columns into one"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ae94145eddab5e6"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def replace_last(string, old, new):\n",
    "    return new.join(string.rsplit(old, 1))\n",
    "\n",
    "def delete_quotation_marks(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        raw_file = f.readlines()\n",
    "        count=0\n",
    "        for i in range(len(raw_file)):\n",
    "            match = re.search(\"\\\"(\\+|-)?\\d+\\.?\\d*,(\\+|-)?\\d+\\.?\\d*\\\"\", raw_file[i])\n",
    "            if match:\n",
    "                count+=1\n",
    "                raw_file[i] = raw_file[i].replace(\"\\\"\",\"\")\n",
    "                raw_file[i] = replace_last(raw_file[i], \",\",\"\\n\")\n",
    "    with open(path, \"w\") as f:\n",
    "        f.writelines(raw_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:52:20.872608700Z",
     "start_time": "2024-01-04T16:52:20.855758100Z"
    }
   },
   "id": "cabc0efb30639685"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "### Load train data\n",
    "df = pd.DataFrame()\n",
    "for root, directory, files in os.walk(\"data/train\", topdown=False):\n",
    "    if files:\n",
    "        for file in files:\n",
    "            tmp = pd.read_csv(f\"{root}/{file}\")\n",
    "            # Remove empty rows and columns\n",
    "            tmp = tmp.dropna(how='all', axis=0)\n",
    "            tmp = tmp.dropna(how='all', axis=1)\n",
    "            # Derive additional columns\n",
    "            tmp[\"league\"] = int(root.split(\"\\\\\")[2])\n",
    "            tmp[\"country\"] = root.split(\"\\\\\")[1]\n",
    "            tmp[\"season\"] = int(file[:2]) # no. of season - 00/01 - 0th season, 21/22 - 21st season\n",
    "            df = pd.concat([df, tmp], axis = 0)\n",
    "\n",
    "### Load test data\n",
    "df_test = pd.DataFrame()\n",
    "for root, directory, files in os.walk(\"data/test\", topdown=False):\n",
    "    if files:\n",
    "        for file in files:\n",
    "            tmp = pd.read_csv(f\"{root}/{file}\")\n",
    "            # Remove empty rows and columns\n",
    "            tmp = tmp.dropna(how='all', axis=0)\n",
    "            tmp = tmp.dropna(how='all', axis=1)\n",
    "            # Derive additional columns\n",
    "            tmp[\"league\"] = int(root.split(\"\\\\\")[2])\n",
    "            tmp[\"country\"] = root.split(\"\\\\\")[1]\n",
    "            tmp[\"season\"] = 22 # no. of season - 00/01 - 0th season, 21/22 - 21st season\n",
    "            df_test = pd.concat([df_test, tmp], axis = 0)\n",
    "\n",
    "df = pd.concat([df, df_test], axis=0, join=\"outer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:52:57.794610600Z",
     "start_time": "2024-01-04T16:52:20.864611300Z"
    }
   },
   "id": "38d7588280b5dd96"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# df[df[\"season\"] == 22 ][\"FTAG\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:52:57.816308Z",
     "start_time": "2024-01-04T16:52:57.794610600Z"
    }
   },
   "id": "a3985957ba26b9a4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df.to_csv(\"combined.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:10.152511200Z",
     "start_time": "2024-01-04T16:52:57.806310700Z"
    }
   },
   "id": "ff8a9a332fc151b0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "delete_quotation_marks(\"combined.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:11.593594500Z",
     "start_time": "2024-01-04T16:53:10.142513300Z"
    }
   },
   "id": "3bdecf73024ac859"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "shutup.please()\n",
    "df = pd.read_csv(\"combined.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:14.540988800Z",
     "start_time": "2024-01-04T16:53:11.594593200Z"
    }
   },
   "id": "13b6c7342ccd09f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data validation\n",
    "\n",
    "In some cases, the data is wrong. This section corrects the loaded data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ba4606dc9c7a36f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Same name for different things\n",
    "\n",
    "In some cases, columns are named differently. We will standardize to use column names that are in *'notes.txt'*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13b8d39a27682c20"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases for HT: data/train\\greece\\1, 0001.csv\n",
      "Cases for AT: data/train\\greece\\1, 0001.csv\n",
      "Cases for HT: data/train\\greece\\1, 0102.csv\n",
      "Cases for AT: data/train\\greece\\1, 0102.csv\n",
      "Cases for HT: data/train\\greece\\1, 0203.csv\n",
      "Cases for AT: data/train\\greece\\1, 0203.csv\n",
      "Cases for HT: data/train\\greece\\1, 0304.csv\n",
      "Cases for AT: data/train\\greece\\1, 0304.csv\n",
      "Cases for HT: data/train\\greece\\1, 0405.csv\n",
      "Cases for AT: data/train\\greece\\1, 0405.csv\n"
     ]
    }
   ],
   "source": [
    "for root, directory, files in os.walk(\"data/train\", topdown=False):\n",
    "    if files:\n",
    "        for file in files:\n",
    "            tmp = pd.read_csv(f\"{root}/{file}\")\n",
    "            if \"HT\" in tmp.columns:\n",
    "                print(f\"Cases for HT: {root}, {file}\")\n",
    "            if \"AT\" in tmp.columns:\n",
    "                print(f\"Cases for AT: {root}, {file}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:17.351819800Z",
     "start_time": "2024-01-04T16:53:14.539989900Z"
    }
   },
   "id": "b38a427d869d5318"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The only affected data are in Greece."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "910562b947564879"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df[\"HomeTeam\"] = df[\"HomeTeam\"].mask(df[\"HT\"].notnull(), df[\"HT\"])\n",
    "df[\"AwayTeam\"] = df[\"AwayTeam\"].mask(df[\"AT\"].notnull(), df[\"AT\"])\n",
    "df = df.drop(columns = [\"HT\", \"AT\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:17.461296600Z",
     "start_time": "2024-01-04T16:53:17.351819800Z"
    }
   },
   "id": "a6ec392c63fe6c76"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df.drop(df[df[\"HomeTeam\"] == df[\"AwayTeam\"]].index, inplace=True) # remove cases where teams play against themselves"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:17.612747Z",
     "start_time": "2024-01-04T16:53:17.461296600Z"
    }
   },
   "id": "9d2d1dcb19e3ce38"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format = 'mixed', dayfirst=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:17.988623400Z",
     "start_time": "2024-01-04T16:53:17.615746800Z"
    }
   },
   "id": "55f66d0e63b7982a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Not imputable data\n",
    "\n",
    "In some cases, missing data cause the data to be unusable and can't be computed. This applies to variables\n",
    "\n",
    "- HomeTeam\n",
    "- AwayTeam"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c551a44639ce963a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "crucial_cols = ['Date', 'HomeTeam', 'AwayTeam']\n",
    "df = df.dropna(subset = crucial_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:18.140507100Z",
     "start_time": "2024-01-04T16:53:17.990622900Z"
    }
   },
   "id": "79a6dc9ad52b014a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "              HFKC         AFKC\ncount  1305.000000  1305.000000\nmean     16.627586    17.283525\nstd       4.386871     4.735901\nmin       2.000000     2.000000\n25%      14.000000    14.000000\n50%      16.000000    17.000000\n75%      20.000000    20.000000\nmax      32.000000    34.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HFKC</th>\n      <th>AFKC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1305.000000</td>\n      <td>1305.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>16.627586</td>\n      <td>17.283525</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>4.386871</td>\n      <td>4.735901</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>14.000000</td>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>16.000000</td>\n      <td>17.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>20.000000</td>\n      <td>20.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>32.000000</td>\n      <td>34.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"HFKC\", \"AFKC\"]].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:18.195261800Z",
     "start_time": "2024-01-04T16:53:18.128508Z"
    }
   },
   "id": "8f63ed5248520abe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Goal validation\n",
    "\n",
    "Some goals are incorrect and they need to be fixed.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4acf7b9deb3b17e"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                FTHG           FTAG           HTHG           HTAG\ncount  154495.000000  154473.000000  154445.000000  154454.000000\nmean        2.217761       1.131447       0.655308       0.494315\nstd        21.887856       1.108842       0.809085       0.708314\nmin         0.000000      -1.000000       0.000000       0.000000\n25%         1.000000       0.000000       0.000000       0.000000\n50%         1.000000       1.000000       0.000000       0.000000\n75%         2.000000       2.000000       1.000000       1.000000\nmax       998.000000      13.000000       7.000000       6.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FTHG</th>\n      <th>FTAG</th>\n      <th>HTHG</th>\n      <th>HTAG</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>154495.000000</td>\n      <td>154473.000000</td>\n      <td>154445.000000</td>\n      <td>154454.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.217761</td>\n      <td>1.131447</td>\n      <td>0.655308</td>\n      <td>0.494315</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>21.887856</td>\n      <td>1.108842</td>\n      <td>0.809085</td>\n      <td>0.708314</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>998.000000</td>\n      <td>13.000000</td>\n      <td>7.000000</td>\n      <td>6.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_cols = [\"FTHG\", \"FTAG\", \"HTHG\", \"HTAG\"]\n",
    "df[goal_cols].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:18.343096200Z",
     "start_time": "2024-01-04T16:53:18.175267400Z"
    }
   },
   "id": "657538d5cf004510"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There seem to be several issues:\n",
    "\n",
    "- the max value for **FTHG** is absurd\n",
    "- the min value for **FTAG** does not make sense\n",
    "- some values are missing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aa520f7268f331d"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "FTHG\n1.0      51639\n2.0      37642\n0.0      36011\n3.0      18462\n4.0       7202\n5.0       2343\n6.0        751\n7.0        185\n8.0         42\n9.0         14\n10.0         5\n339.0        5\n373.0        4\n772.0        4\n572.0        4\nName: count, dtype: int64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"FTHG\"].value_counts().head(15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:18.344096Z",
     "start_time": "2024-01-04T16:53:18.249452400Z"
    }
   },
   "id": "3a2d1f300c063570"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we will remove all rows where the Full Time Home Goals are greater than 15."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c69ffe8dd5a43a13"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "df_22 = df[df[\"season\"] == 22] # this split is necessary as goal values are unknown for test season\n",
    "df = df[df[\"FTHG\"] <= 15] # this line gets also rid of all NaNs\n",
    "df = pd.concat([df, df_22])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:18.631959800Z",
     "start_time": "2024-01-04T16:53:18.276117800Z"
    }
   },
   "id": "7c3d6a035bd61f5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, all Full Time Away goals that are less than zero are inspected. Since we are not sure which values are good and bad, all of them are removed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2334a5cf5203f897"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# df_22 = df[df[\"season\"] == 22] # keep season 22\n",
    "df = df[df[\"FTAG\"] >= 0]\n",
    "df = pd.concat([df, df_22])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:18.784542200Z",
     "start_time": "2024-01-04T16:53:18.519996600Z"
    }
   },
   "id": "222e2ecfc432def5"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "7277"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"FTHG\"].isna().sum() # there should be 7277, which is the count of rows in season 22"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:18.793554900Z",
     "start_time": "2024-01-04T16:53:18.786087100Z"
    }
   },
   "id": "cad1888faa5c1492"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "7277"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"FTAG\"].isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:53:18.820055400Z",
     "start_time": "2024-01-04T16:53:18.793554900Z"
    }
   },
   "id": "4a38347e679a7df0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "What's left are the missing values in 'HTHG' and 'HTAG' columns. We have no way of recomputing these as well and these rows are again excluded."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74ea71a7360aa200"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = [\"HTHG\", \"HTAG\"]\n",
    "df_22 = df[df[\"season\"] == 22]\n",
    "df = df[df[cols].isna().sum(axis=1) == 0]\n",
    "df = pd.concat([df, df_22], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "348f03199821e263"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we run the describe() function again, we can see that the counts in all columns match and min/max statistics make sense."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80a4fcb28b6de737"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[goal_cols].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c2d81edb999419"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shots on target imputation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9d528dd4fc6b5e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lm = PoissonRegressor()\n",
    "\n",
    "y_train = df[df[\"season\"] != 22][[\"AST\", \"HST\"]]\n",
    "X_train = df[df[\"season\"] != 22][[\"AST\", \"HST\", \"FTAG\", \"FTHG\"]]\n",
    "X_train.dropna(subset=[\"AST\", \"HST\"],axis=0, inplace=True, how='any')\n",
    "y_train.dropna(axis=0, inplace=True, how='any')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6a31a5e4b95158"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train[\"FTAG\"],X_train[\"FTHG\"]], axis=0)\n",
    "y_train = pd.concat([y_train[\"AST\"], y_train[\"HST\"]], axis=0)\n",
    "\n",
    "X_train = np.array(X_train).reshape(-1,1)\n",
    "y_train = np.array(y_train).reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2709ab93fbac7aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3f026cebf582797"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_HST = df[(df[\"season\"] != 22) & (df[\"HST\"].isna())][[\"HST\", \"FTHG\"]]\n",
    "\n",
    "X_pred_AST = df[(df[\"season\"] != 22) & (df[\"AST\"].isna())][[\"AST\", \"FTAG\"]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22ebf39913207833"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_AST =   np.array(X_pred_AST[\"FTAG\"]).reshape(-1,1)\n",
    "X_pred_HST =  np.array(X_pred_HST[\"FTHG\"]).reshape(-1,1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd2adbae25835ebc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_HST = lm.predict(X_pred_HST)\n",
    "y_pred_AST = lm.predict(X_pred_AST)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1095855018ff49b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_HST = df[df[\"season\"]!= 22][[\"HST\", \"FTHG\"]]\n",
    "X_pred_AST = df[df[\"season\"]!= 22][[\"AST\", \"FTAG\"]]\n",
    "X_pred_HST = X_pred_HST[X_pred_HST.isna().any(axis=1)]\n",
    "X_pred_AST = X_pred_AST[X_pred_AST.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "777343b5f0066199"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_AST[\"AST\"] = y_pred_AST\n",
    "X_pred_HST[\"HST\"] = y_pred_HST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cee9294fb093dda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_HST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e5ca4b06cd4a7d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[(df[\"season\"] != 22) & (df[\"HST\"].isna())][\"HST\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4afdf7be59a5cc84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[(df[\"season\"] != 22) & (df[\"AST\"].isna()), \"AST\"] = X_pred_AST[\"AST\"]\n",
    "df.loc[(df[\"season\"] != 22) & (df[\"HST\"].isna()), \"HST\"] = X_pred_HST[\"HST\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d10a4d61600d666"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"season\"] != 22][\"HST\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6112f57737b98c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result validation\n",
    "\n",
    "Another thing that needs to be validated is the result classification, which are re-classified - this is the easiest data validation process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef3eaed669ce84b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "half_conds = [df[\"HTHG\"] > df[\"HTAG\"], df[\"HTHG\"] < df[\"HTAG\"], df[\"HTHG\"] == df[\"HTAG\"]]\n",
    "half_choic = [\"H\"                    , \"A\"                    , \"D\"]\n",
    "df.loc[:, \"HTR\"] = np.select(half_conds, half_choic)\n",
    "full_conds = [df[\"FTHG\"] > df[\"FTAG\"], df[\"FTHG\"] < df[\"FTAG\"], df[\"FTHG\"] == df[\"FTAG\"]]\n",
    "full_choic = [\"H\"                    , \"A\"                    , \"D\"]\n",
    "df.loc[:, \"FTR\"] = np.select(full_conds, full_choic)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ecb2c18b7b9f8f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation of other statistics\n",
    "\n",
    "First step is to look at simple descriptive statistics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b81ba4f6d2977f12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stat_cols = [\n",
    "    'Attendance', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW',\n",
    "    'HC', 'AC', 'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', \"HBP\", \"ABP\"\n",
    "]\n",
    "\n",
    "df[stat_cols].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecb804ccb7cb8dee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we take a look at boxplots for variables where at least one variable falls out of the interval\n",
    "\n",
    "$$\n",
    "(\\text{q}_{0.25} - 3 * \\text{IQR} ; \\text{q}_{0.75} + 3 * \\text{IQR})\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ababdeb967a6e24d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def outliers(x, multi = 3):\n",
    "    q25 = x.quantile(0.25)\n",
    "    q75 = x.quantile(0.75)\n",
    "    iqr = q75 - q25\n",
    "    outliers = x[(x < q25 - multi * iqr) | (x > q75 + multi * iqr)]\n",
    "    return len(outliers)\n",
    "\n",
    "stat_outliers = df[stat_cols].apply(lambda x: outliers(x))\n",
    "stat_outliers = stat_outliers[stat_outliers != 0]\n",
    "stat_outliers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36f4cd242adad8dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols = 6, figsize = (12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(stat_outliers.index):\n",
    "    # Data\n",
    "    df[col].plot(kind='box', ax = axes[i])\n",
    "    # Styling\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].axes.get_xaxis().set_visible(False)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc441d0287e11da0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can se that some outliers are not that extreme. For example variables *HO*, *AO* or *HY* have extreme values, but won't remove them. This is because even tho they can be considered as extreme, there seems to be a natural way how they occured and tehre are no huge jumps between them.\n",
    "\n",
    "On the other hand, variables *AHW*, *HF*, *AF* and *AR* seem to have some variables that are far away from the other data. Note that we do not consider variables in *HR* as outliers because it looks like it's a discrete variable with mostly zeroes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b436c85ccbb219ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stat_jump = [\n",
    "    \"AS\", \"AHW\", \"HF\", \"AF\", \"AR\", \"ABP\"\n",
    "]\n",
    "stats_cut = [\n",
    "    38, 6, 100, 60, 6, 130\n",
    "]\n",
    "\n",
    "print(\"Number of extreme values:\")\n",
    "for i, col in enumerate(stat_jump):\n",
    "    cutoff = stats_cut[i]\n",
    "    x = df[col].to_numpy()\n",
    "    extreme = x[x > cutoff]\n",
    "    print(f\"{col}: {extreme}, (count: {len(extreme)})\")\n",
    "    \n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46eb2dec664ff5ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the analysis, we see that these extreme values occur at most twice. This is why we chose to remove them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbf8a0ae4c9369f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criteria = dict(zip(stat_jump, stats_cut))\n",
    "for column, value in criteria.items():\n",
    "    # .isna() is important because otherwise all na rows are dropped\n",
    "    df = df[df[column].lt(value) | df[column].isna()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55fb51150389ada2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we will double-check with boxplots."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4bf92a400a5c622"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols = 3, figsize = (8, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(stat_jump):\n",
    "    df[col].plot(kind='box', ax = axes[i])\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].axes.get_xaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c2c0f46747e2616"
  },
  {
   "cell_type": "markdown",
   "source": [
    "IT looks like variable *HF* contains another outlier, but this process should be done only once. Hence, no outliers are removed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b51cae9758e8274"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random unnamed data\n",
    "\n",
    "File *'data/train/portugal/1/0304.csv'* contains random data in columns *'Unnamed: 33'* and *'Unnamed: 34'*."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "616fbd2dd4ae950a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unnamed_cols_df = df[['Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34']]\n",
    "unnamed_cols_df[unnamed_cols_df.notnull().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31cc4459ca7cdf22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we do not know what these columns represent, and it is only one non-NA row from the whole dataset, this column is removed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e33c76eebd3bfd49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(columns = unnamed_cols_df.columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4747ed1cab9b83cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrong betting odds names\n",
    "\n",
    "File *'data/train/germany/2/0405.csv'* contains columns **LB**, **LB.1** and **LB.2**, which are unique only to this file. After further investigation, they represent the betting odds data for Ladbrokers. After looking at the data more thoroughly, it can be guessed that all three columns represent odds for home win, away win, and draw."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95611e75adfeb7e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(\"data/train/germany/2/0405.csv\")\n",
    "# Remove empty rows and columns\n",
    "tmp = tmp.dropna(how='all', axis=0)\n",
    "tmp = tmp.dropna(how='all', axis=1)\n",
    "tmp = tmp.loc[:, ~tmp.columns.str.startswith('Unnamed:')]\n",
    "tmp = tmp[tmp[['LB', 'LB.1', 'LB.2']].notnull().any(axis=1)]\n",
    "tmp.filter(regex='[HDAB12]$').iloc[:, -12:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28b3e99c9ab8939"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on column similarity, we can make an edjucated guess that\n",
    "\n",
    "- **LB** should be **LBH**,\n",
    "- **LB.1** should be **LBD**, and\n",
    "- **LB.2** should be **LBA**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d9e9ec35834c3d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If LB is not null, use that value and replace it in LBH\n",
    "df[\"LBH\"] = df[\"LBH\"].mask(df[\"LB\"  ].notnull(), df[\"LB\"])\n",
    "df[\"LBD\"] = df[\"LBD\"].mask(df[\"LB.1\"].notnull(), df[\"LB.1\"])\n",
    "df[\"LBA\"] = df[\"LBA\"].mask(df[\"LB.2\"].notnull(), df[\"LB.2\"])\n",
    "df = df.drop(columns = [\"LB\", \"LB.1\", \"LB.2\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64d2fcc5cf63cfc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Date normalization\n",
    "\n",
    "Date is not consistent and it needs to be unified in order to format it as date."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca8a693524100ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "potential_fixes = pd.to_datetime(df['Date'], format='mixed', dayfirst=True)\n",
    "equal = potential_fixes[potential_fixes.isnull()].index.equals(df[df[\"Date\"].isnull()].index)\n",
    "# if True, all non-na dates have been converted\n",
    "if equal:\n",
    "    df[\"Date\"] = potential_fixes\n",
    "else:\n",
    "   print(\"Date indexes are not equal. Something wrong with the conversion??\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb3d7270366eb7f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bookies analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17dbcca0e54297f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Betting odds validation (betting odds have to be positive)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eb431954327b99b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "odds_cols = ['GBH', 'GBD', 'GBA', 'IWH', 'IWD', 'IWA', 'SBH', 'SBD', 'SBA', 'WHH', 'WHD', 'WHA', 'B365CH', 'B365CD', 'B365CA', 'BWCH', 'BWCD', 'BWCA', 'IWCH', 'IWCD', 'IWCA', 'WHCH', 'WHCD', 'WHCA', 'VCCH', 'VCCD', 'VCCA', 'MaxCH', 'MaxCD', 'MaxCA', 'AvgCH','AvgCD', 'AvgCA', 'B365C>2.5', 'B365C<2.5', 'PC>2.5', 'PC<2.5', 'MaxC>2.5', 'MaxC<2.5', 'AvgC>2.5', 'AvgC<2.5', 'AHCh', 'B365CAHH', 'B365CAHA', 'PCAHH', 'PCAHA', 'MaxCAHH', 'MaxCAHA', 'AvgCAHH', 'AvgCAHA']\n",
    "df[odds_cols].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0fe84f70c15e442"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# only column 'AHCh' has negative values -> no need to run on all 50 columns\n",
    "df[\"AHCh\"] = abs(df[\"AHCh\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74d1bbf60a5c66e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Count averages and vars of all known odds for home win / away win / draw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cae8fd1e8d935f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#minor warning suppression\n",
    "shutup.please()\n",
    "\n",
    "away_odds = df[[\"B365A\", \"BSA\", \"BWA\", \"GBA\", \"IWA\", \"LBA\", \"PSA\", \"SOA\", \"SBA\", \"SJA\", \"SYA\", \"VCA\", \"WHA\"]]\n",
    "home_odds = df[[\"B365H\", \"BSH\", \"BWH\", \"GBH\", \"IWH\", \"LBH\", \"PSH\", \"SOH\", \"SBH\", \"SJH\", \"SYH\", \"VCH\", \"WHH\"]]\n",
    "draw_odds = df[[\"B365D\", \"BSD\", \"BWD\", \"GBD\", \"IWD\", \"LBD\", \"PSD\", \"SOD\", \"SBD\", \"SJD\", \"SYD\", \"VCD\", \"WHD\"]]\n",
    "\n",
    "#Average of away/home/draw odds\n",
    "df[\"Avg_away_odds\"] = away_odds.mean(axis=1)\n",
    "df[\"Avg_home_odds\"] = home_odds.mean(axis=1)\n",
    "df[\"Avg_draw_odds\"] = draw_odds.mean(axis=1)\n",
    "\n",
    "#Predcition based on averages - Odds with smallest average have the highest probability -> returns A/H/D\n",
    "df[\"Avg_bookie_prediction\"] = df[[\"Avg_away_odds\", \"Avg_home_odds\", \"Avg_draw_odds\"]].idxmin(axis=1).fillna(\"\").astype(str).str[4]\n",
    "df[\"Avg_bookie_prediction\"] = df[\"Avg_bookie_prediction\"].str.upper()\n",
    "\n",
    "\n",
    "# if one team is heavily favoured, there could be a greater amount of goals scored ???\n",
    "df[\"heavy_favour\"] = df[\"Avg_home_odds\"] - df[\"Avg_away_odds\"]\n",
    "\n",
    "#Certainity of odds, the smaller variance implies, that bookies are more \"sure\"\n",
    "df[\"Var_away_odds\"] = away_odds.var(axis=1)\n",
    "df[\"Var_home_odds\"] = home_odds.var(axis=1)\n",
    "df[\"Var_draw_odds\"] = draw_odds.var(axis=1)\n",
    "\n",
    "print(df.loc[:, \"Avg_away_odds\":\"Var_draw_odds\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "717ba7782728df86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"heavy_favour\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f8c93aaa636ed6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Columns for predictions of all bookies\n",
    "bookies_predictions = pd.DataFrame(columns = [\"B365P\", \"BSP\", \"BWP\", \"GBP\", \"IWP\", \"LBP\", \"PSP\", \"SOP\", \"SBP\", \"SJP\", \"SYP\", \"VCP\", \"WHP\"])\n",
    "\n",
    "#dataframe with away/home/draw odds for each bookie\n",
    "df_bookies_accuracy = pd.concat([away_odds, home_odds, draw_odds, bookies_predictions], axis = 1).sort_index(axis = 1)\n",
    "df_bookies_accuracy[\"Outcome\"] = df[[\"FTR\"]]\n",
    "\n",
    "seq = list(range(3, 53, 4))\n",
    "#\"prediction\" of each bookie based on their odds\n",
    "for i in seq:\n",
    "    df_bookies_accuracy.iloc[:, i] = df_bookies_accuracy.iloc[:, i-3:i].idxmin(axis=1).fillna(\"\").astype(str).str[-1]\n",
    "\n",
    "print(df_bookies_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a59ae98bfa347167"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Accuracy of each bookie - all around 50% - accuracy of a bookie cannot be used as a weight for prediction\n",
    "for bookie in bookies_predictions:\n",
    "    filter_df = df_bookies_accuracy[df_bookies_accuracy[bookie].notna()]\n",
    "    matching_values = (filter_df[bookie] == filter_df['Outcome']).sum()\n",
    "    total_values = len(filter_df)\n",
    "    percent= (matching_values/total_values) *100\n",
    "    print(\"Percentage of matching values for \" + bookie[:-1] + f\": {percent:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3cc7ff3f36d9b2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# average bookie prediction --> dummy variables\n",
    "df = pd.get_dummies(df, prefix='Avg_pred', columns=['Avg_bookie_prediction'])\n",
    "df.drop(['Avg_pred_D'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4b2b0adfc192d06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- - -"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34df0c206cd7463"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d973167f813f4f19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Handicap values**\n",
    "look through handicap values - they differ in most columns (if they are present) so the odds values cannot be aggregated"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "141189a175c4cdd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "handicap_cols = ['GBAH', 'B365AH', 'BbAHh', 'AHh', 'LBAH', 'AHCh']\n",
    "df[handicap_cols].sort_values(by='GBAH')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "699b30643ef0e8d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in handicap_cols:\n",
    "    print(df[col].isna().sum() * 100 / len(df[col]), ' % of ', col, 'are NaN')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d42634d0eae8c2d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are **A LOT** of values missing; these cannot be imputed as each company uses different handicap value and their odds ratios are not interchangable. All columns containing handicap values or odds are dropped."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "280a1e3d7a96c693"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop handicap cols\n",
    "df.drop(handicap_cols, axis=1, inplace=True)\n",
    "# drop handicap odds cols\n",
    "handicap_odds = ['B365AHA', 'B365AHH', 'GBAHA', 'GBAHH', 'BbMxAHH', 'BbAvAHH', 'BbMxAHA', 'BbAvAHA', 'LBAHH', 'LBAHA']\n",
    "df.drop(handicap_odds, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a968f579efb5cb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Over 2.5 total goals**\n",
    "we are interested in odds or market averages of odds (not max or closing odds)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1d86a6e21130c80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "over = [\"BbAv>2.5\", \"GB>2.5\", \"B365>2.5\", \"P>2.5\", \"Avg>2.5\"]\n",
    "\n",
    "over_odds = df[over]\n",
    "over_avgs = over_odds.mean(axis=1, skipna=True)\n",
    "\n",
    "print(over_avgs.isna().sum() * 100 / len(over_avgs), \" % of all rows are NaN\")\n",
    "\n",
    "print(over_avgs.var())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfb8b47ec6261b14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop(over, axis=1, inplace=True)\n",
    "# the same with under 2.5 goals and Mx\n",
    "df.drop([\"BbMx>2.5\", \"BbMx<2.5\", \"BbAv<2.5\", \"GB<2.5\", \"B365<2.5\", \"P<2.5\", \"Max>2.5\", \"Max<2.5\", \"Avg<2.5\",\"B365C>2.5\", \"B365C<2.5\", \"PC>2.5\", \"PC<2.5\", \"MaxC>2.5\", \"MaxC<2.5\", \"AvgC>2.5\", \"AvgC<2.5\"], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec4b11df5d8fc30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "since 11.5 % are missing, we would have to impute them (probably mean); low var means the values are very close to each other and would be of no use to have similar values in data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b822cc520289a5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop all completely empty columns and rows which there is a lot of, dropped 41 empty columns in total."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ac8438a6f2c34f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dropna(how='all', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c02a66fc0d38c055"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5548ab6977af2d52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(df[df[\"season\"]==22])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb262b93b13bca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Exploration\n",
    "\n",
    "Total overview of all variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7aa273852e7c88e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbd5102cc474334a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Percentage of missing values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a95a9cb1190f3457"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "na_vals = df[df[\"season\"] != 22].isna().sum()       # AVOID DATA LEAKAGE ;)\n",
    "na_vals = na_vals/df[df[\"season\"] != 22].shape[0]\n",
    "na_vals.sort_values(ascending = False).head(20)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e885fa3c5a77e5dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# Get first and last occurrences\n",
    "first_occurrences = df.apply(lambda x: x.first_valid_index())\n",
    "last_occurrences = df.apply(lambda x: x.last_valid_index())\n",
    "\n",
    "missing_counts = {}\n",
    "\n",
    "for column in df.columns:\n",
    "    # Indexes of the first and last occurrences\n",
    "    first_idx = df[column].first_valid_index()\n",
    "    last_idx = df[column].last_valid_index()\n",
    "\n",
    "    # Select the range between first and last occurrence, count missing values\n",
    "    missing_count = df[column][first_idx:last_idx].isnull().sum()\n",
    "    missing_counts[column] = missing_count\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'Variable':          df.columns,\n",
    "    'First Occurrence':  df[\"Date\"].to_numpy()[first_occurrences.values],\n",
    "    'Last Occurrence':   df[\"Date\"].to_numpy()[last_occurrences.values],\n",
    "    \"Missing within\": missing_counts.values()\n",
    "})\n",
    "\n",
    "result_df.sort_values(by=\"Missing within\", ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31af5580a04824c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "We decided to add several features to the dataset.\n",
    "These features are:\n",
    "- Result of the last match between the two contending teams\n",
    "- Goal score during the last match between the two contending teams\n",
    "- Average amount of goals scored in the current season\n",
    "- Average amount of goals received in the current season\n",
    "\n",
    "We believe that these features will prove useful in the training of our model as they can reveal things such as momentum and strenghts/weaknesses against certain teams."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47b9197d698b0884"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Result of the last match between the two contending teams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "267d7f95364d10c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a string of participating teams, append them alphabetically behind each other so it is easier to slice them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a900fb70ca4b9f2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"Index\"] = df.index\n",
    "df[\"MatchTeams\"] = df[[\"HomeTeam\",\"AwayTeam\"]].values.tolist()\n",
    "df[\"MatchTeams\"] = df[\"MatchTeams\"].sort_values().apply(lambda x: sorted(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "893020510b4ea1ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.assign(MatchTeams=df[\"MatchTeams\"].apply(lambda l: \"_\".join(l)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8fab8f28313a46c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.sort_values(['MatchTeams','Date'],ascending=True).groupby('MatchTeams').shift()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f01e76f6fd9ab0c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c9bb5d377391e1b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use groupby to group by same matches, create [\"LastMatchIndex\",\"LastMatchAwayGoals\", \"LastMatchHomeGoals\"] columns with unsorted values.\n",
    "First match of two teams gets empty column index as LastMatchIndex"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ace2945a58591921"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[[\"LastMatchIndex\",\"LastMatchAwayGoals\", \"LastMatchHomeGoals\"]] = df.sort_values(['MatchTeams','Date'],ascending=True).groupby('MatchTeams').shift()[[\"Index\",\"FTAG\", \"FTHG\"]]\n",
    "df.loc[np.isnan(df[\"LastMatchIndex\"]), \"LastMatchIndex\"] = len(df.index)-1\n",
    "df.loc[len(df.index)] = [np.nan for _ in range(df.shape[1])]\n",
    "df.loc[len(df.index)-1]\n",
    "df[\"LastMatchIndex\"] = df[\"LastMatchIndex\"].replace(np.nan, len(df.index)-1)\n",
    "\n",
    "df[\"LastMatchIndex\"].fillna(len(df.index)-1)\n",
    "arr = df[\"LastMatchIndex\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e19061c61614b08"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Switch LastMatchHomeGoals and LastMatchAwayGoals if they do not correspond to the teams accordingly, calculate who won the match"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9953915b23e0e78e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"SameHomeTeam\"] = (df.iloc[arr][\"HomeTeam\"].values == df[\"HomeTeam\"].values)\n",
    "df.loc[df[\"SameHomeTeam\"],['LastMatchHomeGoals','LastMatchAwayGoals']] = df.loc[df[\"SameHomeTeam\"],['LastMatchHomeGoals','LastMatchAwayGoals']].values\n",
    "df[\"LastMatchAwayWin\"] = (df[\"LastMatchAwayGoals\"] > df[\"LastMatchHomeGoals\"]).astype(int)\n",
    "df[\"LastMatchHomeWin\"] = (df[\"LastMatchAwayGoals\"] < df[\"LastMatchHomeGoals\"]).astype(int)\n",
    "df[\"LastMatchDraw\"] = (df[\"LastMatchAwayGoals\"] == df[\"LastMatchHomeGoals\"]).astype(int)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef37e7c3ec7c7106"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop([\"SameHomeTeam\", \"LastMatchIndex\", \"Index\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1eccf66d4b55cf8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"MatchTeams\"] == \"Chelsea_Liverpool\"][[\"Date\",\"FTHG\", \"FTAG\", \"LastMatchHomeGoals\", \"LastMatchAwayGoals\", \"LastMatchHomeWin\",\"LastMatchAwayWin\", \"LastMatchDraw\"]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "702e71a77816a6db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split data back to individual files based on countries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "529da11afee3b680"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"season\"] == 22]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9b66c0508d1dd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "land_list = df[\"country\"].unique()[:-1]\n",
    "for country in land_list:\n",
    "    dfs[f\"df_{country}\"] = df[df[\"country\"] == country]\n",
    "    dfs[f\"df_{country}\"].dropna(axis=1, inplace=True, how=\"all\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96e5669db81fd89a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dummies from teams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45dfb67b476bde31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for country in dfs:\n",
    "    tmp = dfs[country][[\"HomeTeam\", \"AwayTeam\"]]\n",
    "    dfs[country] = pd.get_dummies(dfs[country], columns=[\"HomeTeam\", \"AwayTeam\", \"Div\", \"league\"])\n",
    "    try:\n",
    "        dfs[country] = pd.get_dummies(dfs[country], columns=[\"Referee\"])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    dfs[country] = pd.concat([dfs[country], tmp], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dae966d75452d15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Average amount of goals scored/received in the earlier matches\n",
    "\n",
    "HomeTeamAvgScored, AwayTeamAvgScored, HomeTeamAvgReceived, AwayTeamAvgReceived\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe67b5862f427756"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ty prumery mozna nesedi (koukala jsem na sezonu 22, ale mozna jsem koukala spatne) - PLS CHECK\n",
    "def get_avg_team(df):\n",
    "    lastmatch_home = pd.Series()\n",
    "    df_sub =  df[df[\"season\"] != 22]\n",
    "    lastmatch_home[\"HomeTeamAvgScored\"] = df_sub[\"HomeTeamAvgScored\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayTeamAvgScored\"] = df_sub[\"AwayTeamAvgScored\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeTeamAvgReceived\"] = df_sub[\"HomeTeamAvgReceived\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayTeamAvgReceived\"] = df_sub[\"AwayTeamAvgReceived\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeTeamAvgShotsOnTarget\"] = df_sub[\"HomeTeamAvgShotsOnTarget\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayTeamAvgShotsOnTarget\"] = df_sub[\"AwayTeamAvgShotsOnTarget\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeWinRatio\"] = df_sub[\"HomeWinRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeLossRatio\"] = df_sub[\"HomeLossRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeDrawRatio\"] = df_sub[\"HomeDrawRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayWinRatio\"] = df_sub[\"HomeWinRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayLossRatio\"] = df_sub[\"HomeLossRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayDrawRatio\"] = df_sub[\"HomeDrawRatio\"].dropna().mean()\n",
    "    return lastmatch_home\n",
    "\n",
    "def get_goals_stats(df):\n",
    "    df.sort_values(by=\"Date\", inplace=True)\n",
    "    df[\"HomeTeamAvgScored\"] = 0\n",
    "    df[\"AwayTeamAvgScored\"] = 0\n",
    "    df[\"HomeTeamAvgReceived\"] = 0\n",
    "    df[\"AwayTeamAvgReceived\"] = 0\n",
    "    \n",
    "    df[\"HomeWinRatio\"] = 0\n",
    "    df[\"HomeLossRatio\"] = 0\n",
    "    df[\"HomeDrawRatio\"] = 0\n",
    "    \n",
    "    df[\"AwayWinRatio\"] = 0\n",
    "    df[\"AwayLossRatio\"] = 0\n",
    "    df[\"AwayDrawRatio\"] = 0\n",
    "  \n",
    "    df[\"HomeTeamAvgShotsOnTarget\"] = 0\n",
    "    df[\"AwayTeamAvgShotsOnTarget\"] = 0\n",
    "   \n",
    "    home_wins = 0\n",
    "    home_losses = 0\n",
    "    home_draws = 0\n",
    "\n",
    "    away_wins = 0\n",
    "    away_losses = 0\n",
    "    away_draws = 0\n",
    "    \n",
    "    team_list = set.union(set(df[\"HomeTeam\"]), set(df[\"AwayTeam\"]))\n",
    "    team_list_received = [f\"{team}_received\" for team in team_list]\n",
    "    team_list_res = [f\"{team}_res\" for team in team_list]\n",
    "    team_list_shots = [f\"{team}_shots\" for team in team_list]\n",
    "\n",
    "    df = df.reindex(df.columns.tolist() + list(team_list) + list(team_list_received) + list(team_list_res) + list(team_list_shots),axis=1)\n",
    "\n",
    "    nancount = 0\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    for i, row in df.iterrows():\n",
    "        home = row[\"HomeTeam\"]\n",
    "        away = row[\"AwayTeam\"]\n",
    "\n",
    "        filtered_df = df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)]\n",
    "        if not filtered_df.empty and (np.isnan(row[\"LastMatchAwayGoals\"]) or np.isnan(row[\"LastMatchHomeGoals\"])):\n",
    "            # print(filtered_df[\"LastMatchAwayGoals\"].iloc[-1])\n",
    "            last_match_away_goals = filtered_df[\"LastMatchAwayGoals\"].iloc[-1]\n",
    "            last_match_home_goals = filtered_df[\"LastMatchHomeGoals\"].iloc[-1]\n",
    "\n",
    "            # print(f\"{last_match_away_goals = }, {last_match_home_goals = }\")\n",
    "            df.loc[i, \"LastMatchAwayGoals\"] = last_match_away_goals if row[\"SameHomeTeam\"] else last_match_home_goals\n",
    "            df.loc[i, \"LastMatchHomeGoals\"] = last_match_home_goals if row[\"SameHomeTeam\"] else last_match_away_goals\n",
    "            # print(f\"{df.loc[i, 'LastMatchAwayGoals'] = }\")\n",
    "            # df.loc[i, \"LastMatchAwayGoals\"] = df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)][\"LastMatchAwayGoals\"].iloc[-1] if row[\"SameHomeTeam\"] else df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)][\"LastMatchHomeGoals\"].iloc[-1]\n",
    "            #\n",
    "            # df.loc[i, \"LastMatchHomeGoals\"] = df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)][\"LastMatchHomeGoals\"].iloc[-1] if row[\"SameHomeTeam\"] else df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)][\"LastMatchAwayGoals\"].iloc[-1]\n",
    "        elif filtered_df.empty:\n",
    "            nancount += 1\n",
    "\n",
    "        ### Pokud testing data -> musime dat posledni dostupna data\n",
    "        if row[\"season\"] == 22:\n",
    "            # print(\"found season 22 :) in row \", i)\n",
    "\n",
    "\n",
    "            lastmatch = df[df[\"season\"] != 22]\n",
    "            try:\n",
    "                lastmatch_home = lastmatch[((df[\"HomeTeam\"] == home) | (lastmatch[\"AwayTeam\"] == home))].iloc[-1]\n",
    "                hometeam = \"Home\" if lastmatch_home[\"HomeTeam\"] == home else \"Away\"\n",
    "\n",
    "            except IndexError:\n",
    "                lastmatch_home = get_avg_team(df)\n",
    "                hometeam = \"Home\"\n",
    "\n",
    "            try:\n",
    "                lastmatch_away = lastmatch[((df[\"HomeTeam\"] == away) | (lastmatch[\"AwayTeam\"] == away))].iloc[-1]\n",
    "                awayteam = \"Away\" if lastmatch_away[\"AwayTeam\"] == away else \"Home\"\n",
    "\n",
    "            except IndexError:\n",
    "                lastmatch_away = get_avg_team(df)\n",
    "                awayteam = \"Away\"            \n",
    "            \n",
    "\n",
    "\n",
    "            df.loc[i, \"HomeTeamAvgScored\"] = lastmatch_home[f\"{hometeam}TeamAvgScored\"]\n",
    "            df.loc[i, \"AwayTeamAvgScored\"] = lastmatch_away[f\"{awayteam}TeamAvgScored\"]\n",
    "            df.loc[i, \"HomeTeamAvgReceived\"] = lastmatch_home[f\"{hometeam}TeamAvgReceived\"]\n",
    "            df.loc[i, \"AwayTeamAvgReceived\"] = lastmatch_away[f\"{awayteam}TeamAvgReceived\"]\n",
    "\n",
    "            df.loc[i, \"HomeTeamAvgShotsOnTarget\"] = lastmatch_home[f\"{hometeam}TeamAvgShotsOnTarget\"]\n",
    "            df.loc[i, \"AwayTeamAvgShotsOnTarget\"] = lastmatch_away[f\"{awayteam}TeamAvgShotsOnTarget\"]\n",
    "\n",
    "\n",
    "            df.loc[i,\"HomeWinRatio\"] = lastmatch_home[f\"{hometeam}WinRatio\"]\n",
    "            df.loc[i,\"HomeLossRatio\"] = lastmatch_home[f\"{hometeam}LossRatio\"]\n",
    "            df.loc[i,\"HomeDrawRatio\"] = lastmatch_home[f\"{hometeam}DrawRatio\"]\n",
    "\n",
    "            df.loc[i,\"AwayWinRatio\"] = lastmatch_home[f\"{awayteam}WinRatio\"]\n",
    "            df.loc[i,\"AwayLossRatio\"] = lastmatch_home[f\"{awayteam}LossRatio\"]\n",
    "            df.loc[i,\"AwayDrawRatio\"] = lastmatch_home[f\"{awayteam}DrawRatio\"]\n",
    "\n",
    "            continue\n",
    "\n",
    "        df.loc[i, \"HomeTeamAvgScored\"] = df[home].dropna().mean()\n",
    "        df.loc[i, \"AwayTeamAvgScored\"] = df[away].dropna().mean()\n",
    "        df.loc[i, \"HomeTeamAvgReceived\"] = df[f\"{home}_received\"].dropna().mean()\n",
    "        df.loc[i, \"AwayTeamAvgReceived\"] = df[f\"{away}_received\"].dropna().mean()\n",
    "\n",
    "        df.loc[i, \"HomeTeamAvgShotsOnTarget\"] = df[f\"{home}_shots\"].dropna().mean()\n",
    "        df.loc[i, \"AwayTeamAvgShotsOnTarget\"] = df[f\"{away}_shots\"].dropna().mean()\n",
    "        \n",
    "        df.loc[i, home] = row[\"FTHG\"]\n",
    "        df.loc[i, f\"{home}_received\"] = row[\"FTAG\"]\n",
    "        df.loc[i, away] = row[\"FTAG\"]\n",
    "        df.loc[i, f\"{away}_received\"] = row[\"FTHG\"]\n",
    "        \n",
    "        df.loc[i, f\"{home}_shots\"] = row[\"HST\"]\n",
    "        df.loc[i, f\"{away}_shots\"] = row[\"AST\"]\n",
    "        \n",
    "        try:\n",
    "            home_wins = df[f\"{home}_res\"].value_counts()[\"W\"]\n",
    "            home_losses = df[f\"{home}_res\"].value_counts()[\"L\"]\n",
    "            home_draws = df[f\"{home}_res\"].value_counts()[\"D\"]\n",
    "    \n",
    "            away_wins = df[f\"{away}_res\"].value_counts()[\"W\"]\n",
    "            away_losses = df[f\"{away}_res\"].value_counts()[\"L\"]\n",
    "            away_draws = df[f\"{away}_res\"].value_counts()[\"D\"]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            df.loc[i,\"HomeWinRatio\"] = home_wins / (home_wins + home_draws + home_losses)\n",
    "            df.loc[i,\"HomeLossRatio\"] = home_losses / (home_wins + home_draws + home_losses)\n",
    "            df.loc[i,\"HomeDrawRatio\"] = home_draws / (home_wins + home_draws + home_losses)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            df.loc[i,\"AwayWinRatio\"] = away_wins / (away_wins + away_draws + away_losses)\n",
    "            df.loc[i,\"AwayLossRatio\"] = away_losses / (away_wins + away_draws + away_losses)\n",
    "            df.loc[i,\"AwayDrawRatio\"] = away_draws / (away_wins + away_draws + away_losses)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        \n",
    "        if row[\"FTR\"] == \"A\":\n",
    "            df.loc[i, f\"{home}_res\"] = \"L\"\n",
    "            df.loc[i, f\"{away}_res\"] = \"W\"\n",
    "        elif row[\"FTR\"] == \"H\":\n",
    "            df.loc[i, f\"{home}_res\"] = \"W\"\n",
    "            df.loc[i, f\"{away}_res\"] = \"L\"\n",
    "        else:\n",
    "            df.loc[i, f\"{home}_res\"] = \"D\"\n",
    "            df.loc[i, f\"{away}_res\"] = \"D\"\n",
    "\n",
    "    display(df[df[\"LastMatchHomeGoals\"].isna()][[\"MatchTeams\", \"LastMatchHomeGoals\", \"LastMatchAwayGoals\"]])\n",
    "    df[df[\"HomeTeamAvgScored\"].isna()][\"HomeTeamAvgScored\"] = 0\n",
    "    df[df[\"AwayTeamAvgScored\"].isna()][\"AwayTeamAvgScored\"] = 0\n",
    "    df[df[\"HomeTeamAvgReceived\"].isna()][\"HomeTeamAvgReceived\"] = 0\n",
    "    df[df[\"HomeTeamAvgReceived\"].isna()][\"HomeTeamAvgReceived\"] = 0\n",
    "    df.drop(team_list, inplace=True, axis=1)\n",
    "    df.drop(team_list_received, inplace=True, axis=1)\n",
    "    df.drop(team_list_res, inplace=True, axis=1)\n",
    "    df.drop(team_list_shots, inplace=True, axis=1)\n",
    "\n",
    "    df.drop([\"index\", \"HomeTeam\", \"AwayTeam\", \"LastMatchIndex\"], inplace=True, axis=1)\n",
    "    display(df[df[\"LastMatchHomeGoals\"].isna()][[\"MatchTeams\", \"LastMatchHomeGoals\", \"LastMatchAwayGoals\"]])\n",
    "    display(nancount)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8185f008d67ca9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for country in dfs:\n",
    "    dfs[country] = get_goals_stats(dfs[country])\n",
    "    # don't get scared if you get \"only\" belgium done, england takes *LONG* (10 min), rest is faster \n",
    "    print(f\"done {country}\")\n",
    "    # break ## UNCOMMENT THIS LINE IF TESTING"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e13eac9316eccdc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add scored/received ratio for each team\n",
    "\n",
    "Problem with NaN and inf values for the first instances where teams have scored 0 or received 0 goals -> we could drop the first season maybe?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f44d09f2a15e8eeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for country in dfs: # this will need fix for season 22? \n",
    "    cnt = dfs[country]\n",
    "    cnt[\"HomeTeamScoredRatio\"] = cnt[\"HomeTeamAvgScored\"]/(cnt[\"HomeTeamAvgReceived\"] + cnt[\"HomeTeamAvgScored\"])\n",
    "    cnt[\"AwayTeamScoredRatio\"] = cnt[\"AwayTeamAvgScored\"]/(cnt[\"AwayTeamAvgReceived\"] + cnt[\"AwayTeamAvgScored\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3858d5e4e2f445c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blg = dfs[\"df_belgium\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9619fe1dad4352a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blg[blg[\"LastMatchHomeGoals\"].isna()][[\"MatchTeams\", \"LastMatchHomeGoals\", \"LastMatchAwayGoals\"]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "954d8430345fec8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blg[blg[\"MatchTeams\"] == \"Antwerp_Westerlo\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb9da5271956ebd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # check if all seasons got through\n",
    "blg[blg[\"season\"] == 22]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5ccf78c19f55ea3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs[]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bf3f409abf310d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drop unnecessary columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c69bf7be15c86bd5"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "combined_labels = [\n",
    "    \"HS\", \"AS\", \"HST\", \"AST\", \"HHW\", \"AHW\",\n",
    "    \"HC\", \"AC\", \"HF\", \"AF\", \"HFKC\", \"AFKC\",\n",
    "    \"HO\", \"AO\", \"HY\", \"AY\", \"HR\", \"AR\",\n",
    "    \"HBP\", \"ABP\",\n",
    "    \"B365H\", \"B365D\", \"B365A\",\n",
    "    \"BSH\", \"BSD\", \"BSA\",\n",
    "    \"BWH\", \"BWD\", \"BWA\",\n",
    "    \"GBH\", \"GBD\", \"GBA\",\n",
    "    \"IWH\", \"IWD\", \"IWA\",\n",
    "    \"LBH\", \"LBD\", \"LBA\",\n",
    "    \"PSH\", \"PSD\", \"PSA\",\n",
    "    \"SOH\", \"SOD\", \"SOA\",\n",
    "    \"SBH\", \"SBD\", \"SBA\",\n",
    "    \"SJH\", \"SJD\", \"SJA\",\n",
    "    \"SYH\", \"SYD\", \"SYA\",\n",
    "    \"VCH\", \"VCD\", \"VCA\",\n",
    "    \"WHH\", \"WHD\", \"WHA\",\n",
    "    \"Bb1X2\", \"BbMxH\", \"BbAvH\", \"BbMxD\", \"BbAvD\", \"BbMxA\", \"BbAvA\",\n",
    "    \"MaxH\", \"MaxD\", \"MaxA\", \"AvgH\", \"AvgD\", \"AvgA\",\n",
    "    \"BbOU\", \"BbMx>2.5\", \"BbAv>2.5\", \"BbMx<2.5\", \"BbAv<2.5\",\n",
    "    \"GB>2.5\", \"GB<2.5\", \"B365>2.5\", \"B365<2.5\", \"P>2.5\", \"P<2.5\",\n",
    "    \"Max>2.5\", \"Max<2.5\", \"Avg>2.5\", \"Avg<2.5\",\n",
    "    \"BbAH\", \"BbAHh\", \"AHh\", \"BbMxAHH\", \"BbAvAHH\", \"BbMxAHA\", \"BbAvAHA\",\n",
    "    \"GBAHH\", \"GBAHA\", \"GBAH\", \"LBAHH\", \"LBAHA\", \"LBAH\",\n",
    "    \"B365AHH\", \"B365AHA\", \"B365AH\", \"PAHH\", \"PAHA\",\n",
    "    \"MaxAHH\", \"MaxAHA\", \"AvgAHH\", \"AvgAHA\", \"Unnamed: 0\", \"Date\", \"FTHG\", \"FTAG\", \"HTHG\", \"FTR\", \"HTHG\", \"HTAG\", \"HTR\", \"Index\", \"country\",\n",
    "    \"PSCH\", \"PSCD\", \"PSCA\", \"Time\", \"B365CH\", \"B365CD\", \"B365CA\",\n",
    "    \"BWCH\", \"BWCD\", \"BWCA\", \"IWCH\", \"IWCD\", \"IWCA\", \"WHCH\", \"WHCD\",\n",
    "    \"WHCA\", \"VCCH\", \"VCCD\", \"VCCA\", \"MaxCH\", \"MaxCD\", \"MaxCA\", \"AvgCH\",\n",
    "    \"AvgCD\", \"AvgCA\", \"B365C>2.5\", \"B365C<2.5\", \"PC>2.5\", \"PC<2.5\",\n",
    "    \"MaxC>2.5\", \"MaxC<2.5\", \"AvgC>2.5\", \"AvgC<2.5\", \"AHCh\", \"B365CAHH\",\n",
    "    \"B365CAHA\", \"PCAHH\", \"PCAHA\", \"MaxCAHH\", \"MaxCAHA\", \"AvgCAHH\", \"AvgCAHA\", \"Attendance\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T17:26:17.511752Z",
     "start_time": "2024-01-04T17:26:17.488759700Z"
    }
   },
   "id": "dc7351b4b5b40e52"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "for country in dfs:\n",
    "    for col in combined_labels:\n",
    "        dfs[country].drop(col,axis=1, inplace=True, errors='ignore')\n",
    "        dfs[country] = dfs[country][dfs[country][\"season\"] != 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T17:26:41.143202Z",
     "start_time": "2024-01-04T17:26:17.498755900Z"
    }
   },
   "id": "c8f894d197f23716"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export Dataset as .csv file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ca8c6e9e8aa759e"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "for country in dfs:\n",
    "    dfs[country][dfs[country][\"season\"] != 22].to_csv(f\"data/train_preprocessed/{country}.csv\")\n",
    "    dfs[country][dfs[country][\"season\"] == 22].to_csv(f\"data/test_preprocessed/{country}.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T17:26:59.333480400Z",
     "start_time": "2024-01-04T17:26:41.147201Z"
    }
   },
   "id": "7515491c70f17e4c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
