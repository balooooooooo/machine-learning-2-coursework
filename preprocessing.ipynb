{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2714e0ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.linear_model import PoissonRegressor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e051ae22ea092cdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load train data\n",
    "\n",
    "- iterate through train data folder \n",
    "- feature engineer the **country** and **league** from parent folder name\n",
    "- join the loaded csv files by rows\n",
    "- removed quotation marks that would in some rows merge two columns into one"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ae94145eddab5e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_last(string, old, new):\n",
    "    return new.join(string.rsplit(old, 1))\n",
    "\n",
    "def delete_quotation_marks(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        raw_file = f.readlines()\n",
    "        count=0\n",
    "        for i in range(len(raw_file)):\n",
    "            match = re.search(\"\\\"(\\+|-)?\\d+\\.?\\d*,(\\+|-)?\\d+\\.?\\d*\\\"\", raw_file[i])\n",
    "            if match:\n",
    "                count+=1\n",
    "                raw_file[i] = raw_file[i].replace(\"\\\"\",\"\")\n",
    "                raw_file[i] = replace_last(raw_file[i], \",\",\"\\n\")\n",
    "    with open(path, \"w\") as f:\n",
    "        f.writelines(raw_file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cabc0efb30639685"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Load train data\n",
    "df = pd.DataFrame()\n",
    "for root, directory, files in os.walk(\"data/train\", topdown=False):\n",
    "    if files:\n",
    "        for file in files:\n",
    "            tmp = pd.read_csv(f\"{root}/{file}\")\n",
    "            # Remove empty rows and columns\n",
    "            tmp = tmp.dropna(how='all', axis=0)\n",
    "            tmp = tmp.dropna(how='all', axis=1)\n",
    "            # Derive additional columns\n",
    "            tmp[\"league\"] = int(root.split(\"\\\\\")[2])\n",
    "            tmp[\"country\"] = root.split(\"\\\\\")[1]\n",
    "            tmp[\"season\"] = int(file[:2]) # no. of season - 00/01 - 0th season, 21/22 - 21st season\n",
    "            df = pd.concat([df, tmp], axis = 0)\n",
    "\n",
    "### Load test data\n",
    "df_test = pd.DataFrame()\n",
    "for root, directory, files in os.walk(\"data/test\", topdown=False):\n",
    "    if files:\n",
    "        for file in files:\n",
    "            tmp = pd.read_csv(f\"{root}/{file}\")\n",
    "            # Remove empty rows and columns\n",
    "            tmp = tmp.dropna(how='all', axis=0)\n",
    "            tmp = tmp.dropna(how='all', axis=1)\n",
    "            # Derive additional columns\n",
    "            tmp[\"league\"] = int(root.split(\"\\\\\")[2])\n",
    "            tmp[\"country\"] = root.split(\"\\\\\")[1]\n",
    "            tmp[\"season\"] = 22 # no. of season - 00/01 - 0th season, 21/22 - 21st season\n",
    "            df_test = pd.concat([df_test, tmp], axis = 0)\n",
    "\n",
    "df = pd.concat([df, df_test], axis=0, join=\"outer\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38d7588280b5dd96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df[df[\"season\"] == 22 ][\"FTAG\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3985957ba26b9a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(\"combined.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff8a9a332fc151b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "delete_quotation_marks(\"combined.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bdecf73024ac859"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shutup.please()\n",
    "df = pd.read_csv(\"combined.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13b6c7342ccd09f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data validation\n",
    "\n",
    "In some cases, the data is wrong. This section corrects the loaded data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ba4606dc9c7a36f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Same name for different things\n",
    "\n",
    "In some cases, columns are named differently. We will standardize to use column names that are in *'notes.txt'*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13b8d39a27682c20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for root, directory, files in os.walk(\"data/train\", topdown=False):\n",
    "    if files:\n",
    "        for file in files:\n",
    "            tmp = pd.read_csv(f\"{root}/{file}\")\n",
    "            if \"HT\" in tmp.columns:\n",
    "                print(f\"Cases for HT: {root}, {file}\")\n",
    "            if \"AT\" in tmp.columns:\n",
    "                print(f\"Cases for AT: {root}, {file}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b38a427d869d5318"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The only affected data are in Greece."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "910562b947564879"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"HomeTeam\"] = df[\"HomeTeam\"].mask(df[\"HT\"].notnull(), df[\"HT\"])\n",
    "df[\"AwayTeam\"] = df[\"AwayTeam\"].mask(df[\"AT\"].notnull(), df[\"AT\"])\n",
    "df = df.drop(columns = [\"HT\", \"AT\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6ec392c63fe6c76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop(df[df[\"HomeTeam\"] == df[\"AwayTeam\"]].index, inplace=True) # remove cases where teams play against themselves"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d2d1dcb19e3ce38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df[\"Date\"] = pd.to_datetime(df[\"Date\"], format = '%d/%m/%Y', dayfirst=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55f66d0e63b7982a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Not imputable data\n",
    "\n",
    "In some cases, missing data cause the data to be unusable and can't be computed. This applies to variables\n",
    "\n",
    "- HomeTeam\n",
    "- AwayTeam"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c551a44639ce963a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crucial_cols = ['Date', 'HomeTeam', 'AwayTeam']\n",
    "df = df.dropna(subset = crucial_cols)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79a6dc9ad52b014a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[[\"HFKC\", \"AFKC\"]].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f63ed5248520abe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Goal validation\n",
    "\n",
    "Some goals are incorrect and they need to be fixed.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4acf7b9deb3b17e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "goal_cols = [\"FTHG\", \"FTAG\", \"HTHG\", \"HTAG\"]\n",
    "df[goal_cols].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "657538d5cf004510"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There seem to be several issues:\n",
    "\n",
    "- the max value for **FTHG** is absurd\n",
    "- the min value for **FTAG** does not make sense\n",
    "- some values are missing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aa520f7268f331d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"FTHG\"].value_counts().head(15)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a2d1f300c063570"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we will remove all rows where the Full Time Home Goals are greater than 15."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c69ffe8dd5a43a13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_22 = df[df[\"season\"] == 22] # this split is necessary as goal values are unknown for test season\n",
    "df = df[df[\"FTHG\"] <= 15] # this line gets also rid of all NaNs\n",
    "df = pd.concat([df, df_22])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c3d6a035bd61f5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, all Full Time Away goals that are less than zero are inspected. Since we are not sure which values are good and bad, all of them are removed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2334a5cf5203f897"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_22 = df[df[\"season\"] == 22] # keep season 22\n",
    "df = df[df[\"FTAG\"] >= 0]\n",
    "df = pd.concat([df, df_22])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "222e2ecfc432def5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"FTHG\"].isna().sum() # there should be 7277, which is the count of rows in season 22"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cad1888faa5c1492"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"FTAG\"].isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a38347e679a7df0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "What's left are the missing values in 'HTHG' and 'HTAG' columns. We have no way of recomputing these as well and these rows are again excluded."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74ea71a7360aa200"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = [\"HTHG\", \"HTAG\"]\n",
    "df_22 = df[df[\"season\"] == 22]\n",
    "# df_pre22 = df[df[\"season\"] != 22]\n",
    "df = df[df[cols].isna().sum(axis=1) == 0]\n",
    "df = pd.concat([df, df_22], axis=0)\n",
    "# df[\"season\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b9a3961024b8d5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we run the describe() function again, we can see that the counts in all columns match and min/max statistics make sense."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8832d12e29a8ce13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[goal_cols].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bbd84ad89c9626a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shots on target imputation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f8e17580598af17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lm = PoissonRegressor()\n",
    "\n",
    "y_train = df[df[\"season\"] != 22][[\"AST\", \"HST\"]]\n",
    "X_train = df[df[\"season\"] != 22][[\"AST\", \"HST\", \"FTAG\", \"FTHG\"]]\n",
    "X_train.dropna(subset=[\"AST\", \"HST\"],axis=0, inplace=True, how='any')\n",
    "y_train.dropna(axis=0, inplace=True, how='any')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c28dadfdafe7d51a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train[\"FTAG\"],X_train[\"FTHG\"]], axis=0)\n",
    "y_train = pd.concat([y_train[\"AST\"], y_train[\"HST\"]], axis=0)\n",
    "\n",
    "X_train = np.array(X_train).reshape(-1,1)\n",
    "y_train = np.array(y_train).reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4466a6b14cb967f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c48eeaf0ca420e68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_HST = df[(df[\"season\"] != 22) & (df[\"HST\"].isna())][[\"HST\", \"FTHG\"]]\n",
    "\n",
    "X_pred_AST = df[(df[\"season\"] != 22) & (df[\"AST\"].isna())][[\"AST\", \"FTAG\"]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c5ad5aa07f91f04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_AST =   np.array(X_pred_AST[\"FTAG\"]).reshape(-1,1)\n",
    "X_pred_HST =  np.array(X_pred_HST[\"FTHG\"]).reshape(-1,1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f12ef5cc91809445"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_HST = lm.predict(X_pred_HST)\n",
    "y_pred_AST = lm.predict(X_pred_AST)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74f9dea5b6125941"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_HST = df[df[\"season\"]!= 22][[\"HST\", \"FTHG\"]]\n",
    "X_pred_AST = df[df[\"season\"]!= 22][[\"AST\", \"FTAG\"]]\n",
    "X_pred_HST = X_pred_HST[X_pred_HST.isna().any(axis=1)]\n",
    "X_pred_AST = X_pred_AST[X_pred_AST.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26ddf4a3308299eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_AST[\"AST\"] = y_pred_AST\n",
    "X_pred_HST[\"HST\"] = y_pred_HST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63d790614220db57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pred_HST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eafbb55876b3653"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[(df[\"season\"] != 22) & (df[\"HST\"].isna())][\"HST\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "549309244bcf0e04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[(df[\"season\"] != 22) & (df[\"AST\"].isna()), \"AST\"] = X_pred_AST[\"AST\"]\n",
    "df.loc[(df[\"season\"] != 22) & (df[\"HST\"].isna()), \"HST\"] = X_pred_HST[\"HST\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5773b872b2defc3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"season\"] != 22][\"HST\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "992a008182b14d6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result validation\n",
    "\n",
    "Another thing that needs to be validated is the result classification, which are re-classified - this is the easiest data validation process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e52e77484568589d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "half_conds = [df[\"HTHG\"] > df[\"HTAG\"], df[\"HTHG\"] < df[\"HTAG\"], df[\"HTHG\"] == df[\"HTAG\"]]\n",
    "half_choic = [\"H\"                    , \"A\"                    , \"D\"]\n",
    "df.loc[:, \"HTR\"] = np.select(half_conds, half_choic)\n",
    "full_conds = [df[\"FTHG\"] > df[\"FTAG\"], df[\"FTHG\"] < df[\"FTAG\"], df[\"FTHG\"] == df[\"FTAG\"]]\n",
    "full_choic = [\"H\"                    , \"A\"                    , \"D\"]\n",
    "df.loc[:, \"FTR\"] = np.select(full_conds, full_choic)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85e5e89fe1176573"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation of other statistics\n",
    "\n",
    "First step is to look at simple descriptive statistics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d09fdf7e0bd7ad56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stat_cols = [\n",
    "    'Attendance', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW',\n",
    "    'HC', 'AC', 'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', \"HBP\", \"ABP\"\n",
    "]\n",
    "\n",
    "df[stat_cols].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc0b4a8d0d72e42f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we take a look at boxplots for variables where at least one variable falls out of the interval\n",
    "\n",
    "$$\n",
    "(\\text{q}_{0.25} - 3 * \\text{IQR} ; \\text{q}_{0.75} + 3 * \\text{IQR})\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45f0423ea6b24818"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def outliers(x, multi = 3):\n",
    "    q25 = x.quantile(0.25)\n",
    "    q75 = x.quantile(0.75)\n",
    "    iqr = q75 - q25\n",
    "    outliers = x[(x < q25 - multi * iqr) | (x > q75 + multi * iqr)]\n",
    "    return len(outliers)\n",
    "\n",
    "stat_outliers = df[stat_cols].apply(lambda x: outliers(x))\n",
    "stat_outliers = stat_outliers[stat_outliers != 0]\n",
    "stat_outliers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "177436f7c3a88243"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols = 6, figsize = (12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(stat_outliers.index):\n",
    "    # Data\n",
    "    df[col].plot(kind='box', ax = axes[i])\n",
    "    # Styling\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].axes.get_xaxis().set_visible(False)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dd75d75ae33862b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can se that some outliers are not that extreme. For example variables *HO*, *AO* or *HY* have extreme values, but won't remove them. This is because even tho they can be considered as extreme, there seems to be a natural way how they occured and tehre are no huge jumps between them.\n",
    "\n",
    "On the other hand, variables *AHW*, *HF*, *AF* and *AR* seem to have some variables that are far away from the other data. Note that we do not consider variables in *HR* as outliers because it looks like it's a discrete variable with mostly zeroes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a888e96f3e2a7992"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stat_jump = [\n",
    "    \"AS\", \"AHW\", \"HF\", \"AF\", \"AR\", \"ABP\"\n",
    "]\n",
    "stats_cut = [\n",
    "    38, 6, 100, 60, 6, 130\n",
    "]\n",
    "\n",
    "print(\"Number of extreme values:\")\n",
    "for i, col in enumerate(stat_jump):\n",
    "    cutoff = stats_cut[i]\n",
    "    x = df[col].to_numpy()\n",
    "    extreme = x[x > cutoff]\n",
    "    print(f\"{col}: {extreme}, (count: {len(extreme)})\")\n",
    "    \n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f21efa2b34676bd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the analysis, we see that these extreme values occur at most twice. This is why we chose to remove them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e02737b46401e115"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criteria = dict(zip(stat_jump, stats_cut))\n",
    "for column, value in criteria.items():\n",
    "    # .isna() is important because otherwise all na rows are dropped\n",
    "    df = df[df[column].lt(value) | df[column].isna()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfd4534b886b19e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we will double-check with boxplots."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa38c764b778db71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols = 3, figsize = (8, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(stat_jump):\n",
    "    df[col].plot(kind='box', ax = axes[i])\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].axes.get_xaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "660bf3ce1d532ff9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "IT looks like variable *HF* contains another outlier, but this process should be done only once. Hence, no outliers are removed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47633077ae692db9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random unnamed data\n",
    "\n",
    "File *'data/train/portugal/1/0304.csv'* contains random data in columns *'Unnamed: 33'* and *'Unnamed: 34'*."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bace25c465e174b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unnamed_cols_df = df[['Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34']]\n",
    "unnamed_cols_df[unnamed_cols_df.notnull().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "786e1f9ac050b5ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we do not know what these columns represent, and it is only one non-NA row from the whole dataset, this column is removed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f14558724384896"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(columns = unnamed_cols_df.columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63344855b69da629"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrong betting odds names\n",
    "\n",
    "File *'data/train/germany/2/0405.csv'* contains columns **LB**, **LB.1** and **LB.2**, which are unique only to this file. After further investigation, they represent the betting odds data for Ladbrokers. After looking at the data more thoroughly, it can be guessed that all three columns represent odds for home win, away win, and draw."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2096a866882ec13c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(\"data/train/germany/2/0405.csv\")\n",
    "# Remove empty rows and columns\n",
    "tmp = tmp.dropna(how='all', axis=0)\n",
    "tmp = tmp.dropna(how='all', axis=1)\n",
    "tmp = tmp.loc[:, ~tmp.columns.str.startswith('Unnamed:')]\n",
    "tmp = tmp[tmp[['LB', 'LB.1', 'LB.2']].notnull().any(axis=1)]\n",
    "tmp.filter(regex='[HDAB12]$').iloc[:, -12:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46c0204ec919c55a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on column similarity, we can make an edjucated guess that\n",
    "\n",
    "- **LB** should be **LBH**,\n",
    "- **LB.1** should be **LBD**, and\n",
    "- **LB.2** should be **LBA**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "353c391dda553e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If LB is not null, use that value and replace it in LBH\n",
    "df[\"LBH\"] = df[\"LBH\"].mask(df[\"LB\"  ].notnull(), df[\"LB\"])\n",
    "df[\"LBD\"] = df[\"LBD\"].mask(df[\"LB.1\"].notnull(), df[\"LB.1\"])\n",
    "df[\"LBA\"] = df[\"LBA\"].mask(df[\"LB.2\"].notnull(), df[\"LB.2\"])\n",
    "df = df.drop(columns = [\"LB\", \"LB.1\", \"LB.2\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62309977e66ec42d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Date normalization\n",
    "\n",
    "Date is not consistent and it needs to be unified in order to format it as date."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "576e514dc2ec2ff9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "potential_fixes = pd.to_datetime(df['Date'], format='mixed', dayfirst=True)\n",
    "equal = potential_fixes[potential_fixes.isnull()].index.equals(df[df[\"Date\"].isnull()].index)\n",
    "# if True, all non-na dates have been converted\n",
    "if equal:\n",
    "    df[\"Date\"] = potential_fixes\n",
    "else:\n",
    "   print(\"Date indexes are not equal. Something wrong with the conversion??\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca78f5ac8fad1e9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bookies analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3ebd7b832fd12f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#minor warning suppression\n",
    "shutup.please()\n",
    "\n",
    "away_odds = df[[\"B365A\", \"BSA\", \"BWA\", \"GBA\", \"IWA\", \"LBA\", \"PSA\", \"SOA\", \"SBA\", \"SJA\", \"SYA\", \"VCA\", \"WHA\"]]\n",
    "home_odds = df[[\"B365H\", \"BSH\", \"BWH\", \"GBH\", \"IWH\", \"LBH\", \"PSH\", \"SOH\", \"SBH\", \"SJH\", \"SYH\", \"VCH\", \"WHH\"]]\n",
    "draw_odds = df[[\"B365D\", \"BSD\", \"BWD\", \"GBD\", \"IWD\", \"LBD\", \"PSD\", \"SOD\", \"SBD\", \"SJD\", \"SYD\", \"VCD\", \"WHD\"]]\n",
    "\n",
    "#Average of away/home/draw odds\n",
    "df[\"Avg_away_odds\"] = away_odds.mean(axis=1)\n",
    "df[\"Avg_home_odds\"] = home_odds.mean(axis=1)\n",
    "df[\"Avg_draw_odds\"] = draw_odds.mean(axis=1)\n",
    "\n",
    "#Predcition based on averages - Odds with smallest average have the highest probability -> returns A/H/D\n",
    "df[\"Avg_bookie_prediction\"] = df[[\"Avg_away_odds\", \"Avg_home_odds\", \"Avg_draw_odds\"]].idxmin(axis=1).fillna(\"\").astype(str).str[4]\n",
    "df[\"Avg_bookie_prediction\"] = df[\"Avg_bookie_prediction\"].str.upper()\n",
    "\n",
    "#Certainity of odds, the smaller variance implies, that bookies are more \"sure\"\n",
    "df[\"Var_away_odds\"] = away_odds.var(axis=1)\n",
    "df[\"Var_home_odds\"] = home_odds.var(axis=1)\n",
    "df[\"Var_draw_odds\"] = draw_odds.var(axis=1)\n",
    "\n",
    "print(df.loc[:, \"Avg_away_odds\":\"Var_draw_odds\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c25515b5503cff13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Columns for predictions of all bookies\n",
    "bookies_predictions = pd.DataFrame(columns = [\"B365P\", \"BSP\", \"BWP\", \"GBP\", \"IWP\", \"LBP\", \"PSP\", \"SOP\", \"SBP\", \"SJP\", \"SYP\", \"VCP\", \"WHP\"])\n",
    "\n",
    "#dataframe with away/home/draw odds for each bookie\n",
    "df_bookies_accuracy = pd.concat([away_odds, home_odds, draw_odds, bookies_predictions], axis = 1).sort_index(axis = 1)\n",
    "df_bookies_accuracy[\"Outcome\"] = df[[\"FTR\"]]\n",
    "\n",
    "seq = list(range(3, 53, 4))\n",
    "#\"prediction\" of each bookie based on their odds\n",
    "for i in seq:\n",
    "    df_bookies_accuracy.iloc[:, i] = df_bookies_accuracy.iloc[:, i-3:i].idxmin(axis=1).fillna(\"\").astype(str).str[-1]\n",
    "\n",
    "print(df_bookies_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "560f2d8c692863de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Accuracy of each bookie - all around 50% - accuracy of a bookie cannot be used as a weight for prediction\n",
    "for bookie in bookies_predictions:\n",
    "    filter_df = df_bookies_accuracy[df_bookies_accuracy[bookie].notna()]\n",
    "    matching_values = (filter_df[bookie] == filter_df['Outcome']).sum()\n",
    "    total_values = len(filter_df)\n",
    "    percent= (matching_values/total_values) *100\n",
    "    print(\"Percentage of matching values for \" + bookie[:-1] + f\": {percent:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab531191012f1bfb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- - -"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fae4bede01492298"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13df9d592fbe9a75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Handicap values fix**\n",
    "Handicaps are written in quarters - divisible by 0.25"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa831713638daa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "handicap_cols = ['GBAH', 'B365AH', 'BbAHh', 'AHh', 'LBAH', 'AHCh']\n",
    "df[handicap_cols].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1843d4a1fd6dc8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# handicaps are usually counted in quarters (ending with .0, .25, .50, .75)\n",
    "for col in handicap_cols:\n",
    "    df = df[(df[col] % 0.25 == 0) | df[col].isna()] # drop rows with numeric value indivisible by 0.25\n",
    "for i in handicap_cols:\n",
    "    print(\n",
    "        \"number of invalid rows left: \", # difference between # of all rows and validated rows\n",
    "        len(df[i]) - len(df[((df[i] % 0.25 == 0) | (df[i].isna()))])\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc46b39a04c4c8c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Betting odds validation (betting odds have to be positive)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb399d7b28eaae5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "odds_cols = ['GBH', 'GBD', 'GBA', 'IWH', 'IWD', 'IWA', 'SBH', 'SBD', 'SBA', 'WHH', 'WHD', 'WHA', 'B365CH', 'B365CD', 'B365CA', 'BWCH', 'BWCD', 'BWCA', 'IWCH', 'IWCD', 'IWCA', 'WHCH', 'WHCD', 'WHCA', 'VCCH', 'VCCD', 'VCCA', 'MaxCH', 'MaxCD', 'MaxCA', 'AvgCH','AvgCD', 'AvgCA', 'B365C>2.5', 'B365C<2.5', 'PC>2.5', 'PC<2.5', 'MaxC>2.5', 'MaxC<2.5', 'AvgC>2.5', 'AvgC<2.5', 'AHCh', 'B365CAHH', 'B365CAHA', 'PCAHH', 'PCAHA', 'MaxCAHH', 'MaxCAHA', 'AvgCAHH', 'AvgCAHA']\n",
    "df[odds_cols].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5a8ae1dff1c7d6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# only column 'AHCh' has negative values -> no need to run on all 50 columns\n",
    "df[\"AHCh\"] = abs(df[\"AHCh\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ee615a107a929b8"
  },
  {
   "cell_type": "markdown",
   "id": "a9ec006d07aaf9ed",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Drop all completely empty columns and rows which there is a lot of, dropped 41 empty columns in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0e94069f87e82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dropna(how='all', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c67e0d03c41e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(df[df[\"season\"]==22])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ffc0436392c3d56"
  },
  {
   "cell_type": "markdown",
   "id": "2f822ca43aff7d4d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Exploration\n",
    "\n",
    "Total overview of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d2dc275b79d0c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b17fe2",
   "metadata": {},
   "source": [
    "Percentage of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0ff92d61fb66f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "na_vals = df[df[\"season\"] != 22].isna().sum()       # AVOID DATA LEAKAGE ;)\n",
    "na_vals = na_vals/df[df[\"season\"] != 22].shape[0]\n",
    "na_vals.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# Get first and last occurrences\n",
    "first_occurrences = df.apply(lambda x: x.first_valid_index())\n",
    "last_occurrences = df.apply(lambda x: x.last_valid_index())\n",
    "\n",
    "missing_counts = {}\n",
    "\n",
    "for column in df.columns:\n",
    "    # Indexes of the first and last occurrences\n",
    "    first_idx = df[column].first_valid_index()\n",
    "    last_idx = df[column].last_valid_index()\n",
    "\n",
    "    # Select the range between first and last occurrence, count missing values\n",
    "    missing_count = df[column][first_idx:last_idx].isnull().sum()\n",
    "    missing_counts[column] = missing_count\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'Variable':          df.columns,\n",
    "    'First Occurrence':  df[\"Date\"].to_numpy()[first_occurrences.values],\n",
    "    'Last Occurrence':   df[\"Date\"].to_numpy()[last_occurrences.values],\n",
    "    \"Missing within\": missing_counts.values()\n",
    "})\n",
    "\n",
    "result_df.sort_values(by=\"Missing within\", ascending=False)"
   ],
   "id": "50b764f244678fc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "We decided to add several features to the dataset.\n",
    "These features are:\n",
    "- Result of the last match between the two contending teams\n",
    "- Goal score during the last match between the two contending teams\n",
    "- Average amount of goals scored in the current season\n",
    "- Average amount of goals received in the current season\n",
    "\n",
    "We believe that these features will prove useful in the training of our model as they can reveal things such as momentum and strenghts/weaknesses against certain teams."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30bec80d7d965594"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Result of the last match between the two contending teams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "731c6074219fca63"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a string of participating teams, append them alphabetically behind each other so it is easier to slice them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb136876c01030c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"Index\"] = df.index\n",
    "df[\"MatchTeams\"] = df[[\"HomeTeam\",\"AwayTeam\"]].values.tolist()\n",
    "df[\"MatchTeams\"] = df[\"MatchTeams\"].sort_values().apply(lambda x: sorted(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90d9139991f1075a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.assign(MatchTeams=df[\"MatchTeams\"].apply(lambda l: \"_\".join(l)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cca557999ee1cea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.sort_values(['MatchTeams','Date'],ascending=True).groupby('MatchTeams').shift()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "471e8ec62aeb1eaa"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b2b3e785cc19563b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use groupby to group by same matches, create [\"LastMatchIndex\",\"LastMatchAwayGoals\", \"LastMatchHomeGoals\"] columns with unsorted values.\n",
    "First match of two teams gets empty column index as LastMatchIndex"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd64750de5803cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[[\"LastMatchIndex\",\"LastMatchAwayGoals\", \"LastMatchHomeGoals\"]] = df.sort_values(['MatchTeams','Date'],ascending=True).groupby('MatchTeams').shift()[[\"Index\",\"FTAG\", \"FTHG\"]]\n",
    "df.loc[np.isnan(df[\"LastMatchIndex\"]), \"LastMatchIndex\"] = len(df.index)-1\n",
    "df.loc[len(df.index)] = [np.nan for _ in range(df.shape[1])]\n",
    "df.loc[len(df.index)-1]\n",
    "df[\"LastMatchIndex\"] = df[\"LastMatchIndex\"].replace(np.nan, len(df.index)-1)\n",
    "\n",
    "df[\"LastMatchIndex\"].fillna(len(df.index)-1)\n",
    "arr = df[\"LastMatchIndex\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24cda30259af6152"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Switch LastMatchHomeGoals and LastMatchAwayGoals if they do not correspond to the teams accordingly, calculate who won the match"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "529fafb307538356"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"SameHomeTeam\"] = (df.iloc[arr][\"HomeTeam\"].values == df[\"HomeTeam\"].values)\n",
    "df.loc[df[\"SameHomeTeam\"],['LastMatchHomeGoals','LastMatchAwayGoals']] = df.loc[df[\"SameHomeTeam\"],['LastMatchHomeGoals','LastMatchAwayGoals']].values\n",
    "df[\"LastMatchAwayWin\"] = (df[\"LastMatchAwayGoals\"] > df[\"LastMatchHomeGoals\"]).astype(int)\n",
    "df[\"LastMatchHomeWin\"] = (df[\"LastMatchAwayGoals\"] < df[\"LastMatchHomeGoals\"]).astype(int)\n",
    "df[\"LastMatchDraw\"] = (df[\"LastMatchAwayGoals\"] == df[\"LastMatchHomeGoals\"]).astype(int)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ea2c9e1455d3e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop([\"SameHomeTeam\", \"LastMatchIndex\", \"Index\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6a2ee9ee9a08c40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"MatchTeams\"] == \"Chelsea_Liverpool\"][[\"Date\",\"FTHG\", \"FTAG\", \"LastMatchHomeGoals\", \"LastMatchAwayGoals\", \"LastMatchHomeWin\",\"LastMatchAwayWin\", \"LastMatchDraw\"]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "168be5a9ad3d3f17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split data back to individual files based on countries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e239b0a6d8d1eca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"season\"] == 22]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a241c05253f729f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "land_list = df[\"country\"].unique()[:-1]\n",
    "for country in land_list:\n",
    "    dfs[f\"df_{country}\"] = df[df[\"country\"] == country]\n",
    "    dfs[f\"df_{country}\"].dropna(axis=1, inplace=True, how=\"all\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63787fad7908ab98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dummies from teams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "198eebfa6e9fe703"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for country in dfs:\n",
    "    tmp = dfs[country][[\"HomeTeam\", \"AwayTeam\"]]\n",
    "    dfs[country] = pd.get_dummies(dfs[country], columns=[\"HomeTeam\", \"AwayTeam\", \"Div\", \"league\"])\n",
    "    try:\n",
    "        dfs[country] = pd.get_dummies(dfs[country], columns=[\"Referee\"])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    dfs[country] = pd.concat([dfs[country], tmp], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3997eb730fce392"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Average amount of goals scored/received in the earlier matches\n",
    "\n",
    "HomeTeamAvgScored, AwayTeamAvgScored, HomeTeamAvgReceived, AwayTeamAvgReceived\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d636c3c0b0d75b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ty prumery mozna nesedi (koukala jsem na sezonu 22, ale mozna jsem koukala spatne) - PLS CHECK\n",
    "def get_avg_team(df):\n",
    "    lastmatch_home = pd.Series()\n",
    "    df_sub =  df[df[\"season\"] != 22]\n",
    "    lastmatch_home[\"HomeTeamAvgScored\"] = df_sub[\"HomeTeamAvgScored\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayTeamAvgScored\"] = df_sub[\"AwayTeamAvgScored\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeTeamAvgReceived\"] = df_sub[\"HomeTeamAvgReceived\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayTeamAvgReceived\"] = df_sub[\"AwayTeamAvgReceived\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeTeamAvgShotsOnTarget\"] = df_sub[\"HomeTeamAvgShotsOnTarget\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayTeamAvgShotsOnTarget\"] = df_sub[\"AwayTeamAvgShotsOnTarget\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeWinRatio\"] = df_sub[\"HomeWinRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeLossRatio\"] = df_sub[\"HomeLossRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"HomeDrawRatio\"] = df_sub[\"HomeDrawRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayWinRatio\"] = df_sub[\"HomeWinRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayLossRatio\"] = df_sub[\"HomeLossRatio\"].dropna().mean()\n",
    "    lastmatch_home[\"AwayDrawRatio\"] = df_sub[\"HomeDrawRatio\"].dropna().mean()\n",
    "    return lastmatch_home\n",
    "\n",
    "def get_goals_stats(df):\n",
    "    df.sort_values(by=\"Date\", inplace=True)\n",
    "    df[\"HomeTeamAvgScored\"] = 0\n",
    "    df[\"AwayTeamAvgScored\"] = 0\n",
    "    df[\"HomeTeamAvgReceived\"] = 0\n",
    "    df[\"AwayTeamAvgReceived\"] = 0\n",
    "    \n",
    "    df[\"HomeWinRatio\"] = 0\n",
    "    df[\"HomeLossRatio\"] = 0\n",
    "    df[\"HomeDrawRatio\"] = 0\n",
    "    \n",
    "    df[\"AwayWinRatio\"] = 0\n",
    "    df[\"AwayLossRatio\"] = 0\n",
    "    df[\"AwayDrawRatio\"] = 0\n",
    "  \n",
    "    df[\"HomeTeamAvgShotsOnTarget\"] = 0\n",
    "    df[\"AwayTeamAvgShotsOnTarget\"] = 0\n",
    "   \n",
    "    home_wins = 0\n",
    "    home_losses = 0\n",
    "    home_draws = 0\n",
    "\n",
    "    away_wins = 0\n",
    "    away_losses = 0\n",
    "    away_draws = 0\n",
    "    \n",
    "    team_list = set.union(set(df[\"HomeTeam\"]), set(df[\"AwayTeam\"]))\n",
    "    team_list_received = [f\"{team}_received\" for team in team_list]\n",
    "    team_list_res = [f\"{team}_res\" for team in team_list]\n",
    "    team_list_shots = [f\"{team}_shots\" for team in team_list]\n",
    "\n",
    "    df = df.reindex(df.columns.tolist() + list(team_list) + list(team_list_received) + list(team_list_res) + list(team_list_shots),axis=1)\n",
    "    \n",
    "    nancount = 0\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    for i, row in df.iterrows():\n",
    "        home = row[\"HomeTeam\"]\n",
    "        away = row[\"AwayTeam\"]\n",
    "\n",
    "        filtered_df = df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)]\n",
    "        if not filtered_df.empty and (np.isnan(row[\"LastMatchAwayGoals\"]) or np.isnan(row[\"LastMatchHomeGoals\"])):\n",
    "            # print(filtered_df[\"LastMatchAwayGoals\"].iloc[-1])\n",
    "            last_match_away_goals = filtered_df[\"LastMatchAwayGoals\"].iloc[-1]\n",
    "            last_match_home_goals = filtered_df[\"LastMatchHomeGoals\"].iloc[-1]\n",
    "\n",
    "            # print(f\"{last_match_away_goals = }, {last_match_home_goals = }\")\n",
    "            df.loc[i, \"LastMatchAwayGoals\"] = last_match_away_goals if row[\"SameHomeTeam\"] else last_match_home_goals\n",
    "            df.loc[i, \"LastMatchHomeGoals\"] = last_match_home_goals if row[\"SameHomeTeam\"] else last_match_away_goals\n",
    "            # print(f\"{df.loc[i, 'LastMatchAwayGoals'] = }\")\n",
    "            # df.loc[i, \"LastMatchAwayGoals\"] = df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)][\"LastMatchAwayGoals\"].iloc[-1] if row[\"SameHomeTeam\"] else df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)][\"LastMatchHomeGoals\"].iloc[-1]\n",
    "            # \n",
    "            # df.loc[i, \"LastMatchHomeGoals\"] = df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)][\"LastMatchHomeGoals\"].iloc[-1] if row[\"SameHomeTeam\"] else df[(df[\"MatchTeams\"] == row[\"MatchTeams\"]) & (df[\"season\"] != 22)][\"LastMatchAwayGoals\"].iloc[-1]\n",
    "        elif filtered_df.empty:\n",
    "            nancount += 1\n",
    "\n",
    "        ### Pokud testing data -> musime dat posledni dostupna data\n",
    "        if row[\"season\"] == 22:\n",
    "            # print(\"found season 22 :) in row \", i)\n",
    "            \n",
    "            \n",
    "            lastmatch = df[df[\"season\"] != 22] \n",
    "            try:\n",
    "                lastmatch_home = lastmatch[((df[\"HomeTeam\"] == home) | (lastmatch[\"AwayTeam\"] == home))].iloc[-1]\n",
    "                hometeam = \"Home\" if lastmatch_home[\"HomeTeam\"] == home else \"Away\"\n",
    "                \n",
    "            except IndexError:\n",
    "                lastmatch_home = get_avg_team(df)\n",
    "                hometeam = \"Home\"\n",
    "                \n",
    "            try:\n",
    "                lastmatch_away = lastmatch[((df[\"HomeTeam\"] == away) | (lastmatch[\"AwayTeam\"] == away))].iloc[-1]\n",
    "                awayteam = \"Away\" if lastmatch_away[\"AwayTeam\"] == away else \"Home\"\n",
    "                \n",
    "            except IndexError:\n",
    "                lastmatch_away = get_avg_team(df)\n",
    "                awayteam = \"Away\"            \n",
    "            \n",
    "            \n",
    "        \n",
    "            df.loc[i, \"HomeTeamAvgScored\"] = lastmatch_home[f\"{hometeam}TeamAvgScored\"]\n",
    "            df.loc[i, \"AwayTeamAvgScored\"] = lastmatch_away[f\"{awayteam}TeamAvgScored\"]\n",
    "            df.loc[i, \"HomeTeamAvgReceived\"] = lastmatch_home[f\"{hometeam}TeamAvgReceived\"]\n",
    "            df.loc[i, \"AwayTeamAvgReceived\"] = lastmatch_away[f\"{awayteam}TeamAvgReceived\"]\n",
    "\n",
    "            df.loc[i, \"HomeTeamAvgShotsOnTarget\"] = lastmatch_home[f\"{hometeam}TeamAvgShotsOnTarget\"]\n",
    "            df.loc[i, \"AwayTeamAvgShotsOnTarget\"] = lastmatch_away[f\"{awayteam}TeamAvgShotsOnTarget\"]\n",
    "\n",
    "\n",
    "            df.loc[i,\"HomeWinRatio\"] = lastmatch_home[f\"{hometeam}WinRatio\"]\n",
    "            df.loc[i,\"HomeLossRatio\"] = lastmatch_home[f\"{hometeam}LossRatio\"]\n",
    "            df.loc[i,\"HomeDrawRatio\"] = lastmatch_home[f\"{hometeam}DrawRatio\"]\n",
    "\n",
    "            df.loc[i,\"AwayWinRatio\"] = lastmatch_home[f\"{awayteam}WinRatio\"]\n",
    "            df.loc[i,\"AwayLossRatio\"] = lastmatch_home[f\"{awayteam}LossRatio\"]\n",
    "            df.loc[i,\"AwayDrawRatio\"] = lastmatch_home[f\"{awayteam}DrawRatio\"]\n",
    "\n",
    "            continue\n",
    "\n",
    "        df.loc[i, \"HomeTeamAvgScored\"] = df[home].dropna().mean()\n",
    "        df.loc[i, \"AwayTeamAvgScored\"] = df[away].dropna().mean()\n",
    "        df.loc[i, \"HomeTeamAvgReceived\"] = df[f\"{home}_received\"].dropna().mean()\n",
    "        df.loc[i, \"AwayTeamAvgReceived\"] = df[f\"{away}_received\"].dropna().mean()\n",
    "\n",
    "        df.loc[i, \"HomeTeamAvgShotsOnTarget\"] = df[f\"{home}_shots\"].dropna().mean()\n",
    "        df.loc[i, \"AwayTeamAvgShotsOnTarget\"] = df[f\"{away}_shots\"].dropna().mean()\n",
    "        \n",
    "        df.loc[i, home] = row[\"FTHG\"]\n",
    "        df.loc[i, f\"{home}_received\"] = row[\"FTAG\"]\n",
    "        df.loc[i, away] = row[\"FTAG\"]\n",
    "        df.loc[i, f\"{away}_received\"] = row[\"FTHG\"]\n",
    "        \n",
    "        df.loc[i, f\"{home}_shots\"] = row[\"HST\"]\n",
    "        df.loc[i, f\"{away}_shots\"] = row[\"AST\"]\n",
    "        \n",
    "        try:\n",
    "            home_wins = df[f\"{home}_res\"].value_counts()[\"W\"]\n",
    "            home_losses = df[f\"{home}_res\"].value_counts()[\"L\"]\n",
    "            home_draws = df[f\"{home}_res\"].value_counts()[\"D\"]\n",
    "    \n",
    "            away_wins = df[f\"{away}_res\"].value_counts()[\"W\"]\n",
    "            away_losses = df[f\"{away}_res\"].value_counts()[\"L\"]\n",
    "            away_draws = df[f\"{away}_res\"].value_counts()[\"D\"]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            df.loc[i,\"HomeWinRatio\"] = home_wins / (home_wins + home_draws + home_losses)\n",
    "            df.loc[i,\"HomeLossRatio\"] = home_losses / (home_wins + home_draws + home_losses)\n",
    "            df.loc[i,\"HomeDrawRatio\"] = home_draws / (home_wins + home_draws + home_losses)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            df.loc[i,\"AwayWinRatio\"] = away_wins / (away_wins + away_draws + away_losses)\n",
    "            df.loc[i,\"AwayLossRatio\"] = away_losses / (away_wins + away_draws + away_losses)\n",
    "            df.loc[i,\"AwayDrawRatio\"] = away_draws / (away_wins + away_draws + away_losses)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        \n",
    "        if row[\"FTR\"] == \"A\":\n",
    "            df.loc[i, f\"{home}_res\"] = \"L\"\n",
    "            df.loc[i, f\"{away}_res\"] = \"W\"\n",
    "        elif row[\"FTR\"] == \"H\":\n",
    "            df.loc[i, f\"{home}_res\"] = \"W\"\n",
    "            df.loc[i, f\"{away}_res\"] = \"L\"\n",
    "        else:\n",
    "            df.loc[i, f\"{home}_res\"] = \"D\"\n",
    "            df.loc[i, f\"{away}_res\"] = \"D\"\n",
    "\n",
    "    display(df[df[\"LastMatchHomeGoals\"].isna()][[\"MatchTeams\", \"LastMatchHomeGoals\", \"LastMatchAwayGoals\"]])\n",
    "    df[df[\"HomeTeamAvgScored\"].isna()][\"HomeTeamAvgScored\"] = 0\n",
    "    df[df[\"AwayTeamAvgScored\"].isna()][\"AwayTeamAvgScored\"] = 0\n",
    "    df[df[\"HomeTeamAvgReceived\"].isna()][\"HomeTeamAvgReceived\"] = 0\n",
    "    df[df[\"HomeTeamAvgReceived\"].isna()][\"HomeTeamAvgReceived\"] = 0\n",
    "    df.drop(team_list, inplace=True, axis=1)\n",
    "    df.drop(team_list_received, inplace=True, axis=1)\n",
    "    df.drop(team_list_res, inplace=True, axis=1)\n",
    "    df.drop(team_list_shots, inplace=True, axis=1)\n",
    "\n",
    "    df.drop([\"index\", \"HomeTeam\", \"AwayTeam\", \"LastMatchIndex\"], inplace=True, axis=1)\n",
    "    display(df[df[\"LastMatchHomeGoals\"].isna()][[\"MatchTeams\", \"LastMatchHomeGoals\", \"LastMatchAwayGoals\"]])\n",
    "    display(nancount)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa73984d9142d137"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for country in dfs:\n",
    "    dfs[country] = get_goals_stats(dfs[country])\n",
    "    # don't get scared if you get \"only\" belgium done, england takes *LONG* (10 min), rest is faster \n",
    "    print(f\"done {country}\")\n",
    "    # break ## UNCOMMENT THIS LINE IF TESTING"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f5bb940d58faa68"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add scored/received ratio for each team\n",
    "\n",
    "Problem with NaN and inf values for the first instances where teams have scored 0 or received 0 goals -> we could drop the first season maybe?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd633a5ebb1865b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for country in dfs: # this will need fix for season 22? \n",
    "    cnt = dfs[country]\n",
    "    cnt[\"HomeTeamScoredRatio\"] = cnt[\"HomeTeamAvgScored\"]/(cnt[\"HomeTeamAvgReceived\"] + cnt[\"HomeTeamAvgScored\"])\n",
    "    cnt[\"AwayTeamScoredRatio\"] = cnt[\"AwayTeamAvgScored\"]/(cnt[\"AwayTeamAvgReceived\"] + cnt[\"AwayTeamAvgScored\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d347800c9a4fe39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blg = dfs[\"df_belgium\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c75349163754345e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blg[blg[\"MatchTeams\"] == \"Antwerp_Westerlo\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a459d30460f7d176"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check if all seasons got through\n",
    "blg[blg[\"season\"] == 22]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c478ca1bd49e9485"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_turkey = dfs[\"df_turkey\"]\n",
    "\n",
    "# For HomeTeam_Gaziantep\n",
    "teams = [\"Hatayspor\", \"Gaziantep\"]\n",
    "\n",
    "for team in teams:\n",
    "    for location in [\"Home\", \"Away\"]:\n",
    "        home_col_prefix = f\"HomeTeam_{team}\" if location == \"Home\" else f\"AwayTeam_{team}\"\n",
    "\n",
    "        # For mean values\n",
    "        df_turkey.loc[(df_turkey[\"Avg_bookie_prediction\"].isna()) & (df_turkey[home_col_prefix]), f\"Avg_home_odds\"] = df_turkey[df_turkey[home_col_prefix]][\"Avg_home_odds\"].mean()\n",
    "        df_turkey.loc[(df_turkey[\"Avg_bookie_prediction\"].isna()) & (df_turkey[home_col_prefix]), f\"Avg_away_odds\"] = df_turkey[df_turkey[home_col_prefix]][\"Avg_away_odds\"].mean()\n",
    "        df_turkey.loc[(df_turkey[\"Avg_bookie_prediction\"].isna()) & (df_turkey[home_col_prefix]), f\"Avg_draw_odds\"] = pd.concat([df_turkey[df_turkey[home_col_prefix]], df_turkey[df_turkey[f\"HomeTeam_{team}\"]], df_turkey[df_turkey[f\"AwayTeam_{team}\"]]])[\"Avg_draw_odds\"].mean()\n",
    "\n",
    "        # For Var values\n",
    "        df_turkey.loc[(df_turkey[\"Avg_bookie_prediction\"].isna()) & (df_turkey[home_col_prefix]), f\"Var_home_odds\"] = df_turkey[df_turkey[home_col_prefix]][\"Var_home_odds\"].mean()\n",
    "        df_turkey.loc[(df_turkey[\"Avg_bookie_prediction\"].isna()) & (df_turkey[home_col_prefix]), f\"Var_away_odds\"] = df_turkey[df_turkey[home_col_prefix]][\"Var_away_odds\"].mean()\n",
    "        df_turkey.loc[(df_turkey[\"Avg_bookie_prediction\"].isna()) & (df_turkey[home_col_prefix]), f\"Var_draw_odds\"] = pd.concat([df_turkey[df_turkey[home_col_prefix]], df_turkey[df_turkey[f\"HomeTeam_{team}\"]], df_turkey[df_turkey[f\"AwayTeam_{team}\"]]])[\"Var_draw_odds\"].mean()\n",
    "\n",
    "\n",
    "teams = [\"Bradford\", \"Mansfield\"]\n",
    "df_england = dfs[\"df_england\"]\n",
    "\n",
    "for team in teams:\n",
    "    for location in [\"Home\", \"Away\"]:\n",
    "        home_col_prefix = f\"HomeTeam_{team}\" if location == \"Home\" else f\"AwayTeam_{team}\"\n",
    "\n",
    "        # For mean values\n",
    "        df_england.loc[(df_england[\"Avg_bookie_prediction\"].isna()) & (df_england[home_col_prefix]), f\"Avg_home_odds\"] = df_england[df_england[home_col_prefix]][\"Avg_home_odds\"].mean()\n",
    "        df_england.loc[(df_england[\"Avg_bookie_prediction\"].isna()) & (df_england[home_col_prefix]), f\"Avg_away_odds\"] = df_england[df_england[home_col_prefix]][\"Avg_away_odds\"].mean()\n",
    "        df_england.loc[(df_england[\"Avg_bookie_prediction\"].isna()) & (df_england[home_col_prefix]), f\"Avg_draw_odds\"] = pd.concat([df_england[df_england[home_col_prefix]], df_england[df_england[f\"HomeTeam_{team}\"]], df_england[df_england[f\"AwayTeam_{team}\"]]])[\"Avg_draw_odds\"].mean()\n",
    "\n",
    "        # For Var values\n",
    "        df_england.loc[(df_england[\"Avg_bookie_prediction\"].isna()) & (df_england[home_col_prefix]), f\"Var_home_odds\"] = df_england[df_england[home_col_prefix]][\"Var_home_odds\"].mean()\n",
    "        df_england.loc[(df_england[\"Avg_bookie_prediction\"].isna()) & (df_england[home_col_prefix]), f\"Var_away_odds\"] = df_england[df_england[home_col_prefix]][\"Var_away_odds\"].mean()\n",
    "        df_england.loc[(df_england[\"Avg_bookie_prediction\"].isna()) & (df_england[home_col_prefix]), f\"Var_draw_odds\"] = pd.concat([df_england[df_england[home_col_prefix]], df_england[df_england[f\"HomeTeam_{team}\"]], df_england[df_england[f\"AwayTeam_{team}\"]]])[\"Var_draw_odds\"].mean()\n",
    "\n",
    "\n",
    "\n",
    "df_england[\"Avg_bookie_prediction\"] = df_england[[\"Avg_away_odds\", \"Avg_home_odds\", \"Avg_draw_odds\"]].idxmin(axis=1).fillna(\"\").astype(str).str[4]\n",
    "df_england[\"Avg_bookie_prediction\"] = df_england[\"Avg_bookie_prediction\"].str.upper()\n",
    "\n",
    "df_turkey[\"Avg_bookie_prediction\"] = df_turkey[[\"Avg_away_odds\", \"Avg_home_odds\", \"Avg_draw_odds\"]].idxmin(axis=1).fillna(\"\").astype(str).str[4]\n",
    "df_turkey[\"Avg_bookie_prediction\"] = df_turkey[\"Avg_bookie_prediction\"].str.upper()\n",
    "\n",
    "dfs[\"df_england\"] = df_england\n",
    "dfs[\"df_turkey\"] = df_turkey"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "657a42a9dfe08dd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_turkey[(df_turkey[\"Avg_bookie_prediction\"].isna()) & (df_turkey[\"HomeTeam_Gaziantep\"])][\"Avg_home_odds\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28eb23b851c5103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_turkey[df_turkey[\"HomeTeam_Gaziantep\"]][\"Avg_home_odds\"].mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4d8191a08a8763e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(df_turkey[df_turkey[\"Avg_away_odds\"].isna()])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "708c82997e79818f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drop unnecessary columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c69bf7be15c86bd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_labels = [\n",
    "    \"HS\", \"AS\", \"HST\", \"AST\", \"HHW\", \"AHW\",\n",
    "    \"HC\", \"AC\", \"HF\", \"AF\", \"HFKC\", \"AFKC\",\n",
    "    \"HO\", \"AO\", \"HY\", \"AY\", \"HR\", \"AR\",\n",
    "    \"HBP\", \"ABP\",\n",
    "    \"B365H\", \"B365D\", \"B365A\",\n",
    "    \"BSH\", \"BSD\", \"BSA\",\n",
    "    \"BWH\", \"BWD\", \"BWA\",\n",
    "    \"GBH\", \"GBD\", \"GBA\",\n",
    "    \"IWH\", \"IWD\", \"IWA\",\n",
    "    \"LBH\", \"LBD\", \"LBA\",\n",
    "    \"PSH\", \"PSD\", \"PSA\",\n",
    "    \"SOH\", \"SOD\", \"SOA\",\n",
    "    \"SBH\", \"SBD\", \"SBA\",\n",
    "    \"SJH\", \"SJD\", \"SJA\",\n",
    "    \"SYH\", \"SYD\", \"SYA\",\n",
    "    \"VCH\", \"VCD\", \"VCA\",\n",
    "    \"WHH\", \"WHD\", \"WHA\",\n",
    "    \"Bb1X2\", \"BbMxH\", \"BbAvH\", \"BbMxD\", \"BbAvD\", \"BbMxA\", \"BbAvA\",\n",
    "    \"MaxH\", \"MaxD\", \"MaxA\", \"AvgH\", \"AvgD\", \"AvgA\",\n",
    "    \"BbOU\", \"BbMx>2.5\", \"BbAv>2.5\", \"BbMx<2.5\", \"BbAv<2.5\",\n",
    "    \"GB>2.5\", \"GB<2.5\", \"B365>2.5\", \"B365<2.5\", \"P>2.5\", \"P<2.5\",\n",
    "    \"Max>2.5\", \"Max<2.5\", \"Avg>2.5\", \"Avg<2.5\",\n",
    "    \"BbAH\", \"BbAHh\", \"AHh\", \"BbMxAHH\", \"BbAvAHH\", \"BbMxAHA\", \"BbAvAHA\",\n",
    "    \"GBAHH\", \"GBAHA\", \"GBAH\", \"LBAHH\", \"LBAHA\", \"LBAH\",\n",
    "    \"B365AHH\", \"B365AHA\", \"B365AH\", \"PAHH\", \"PAHA\",\n",
    "    \"MaxAHH\", \"MaxAHA\", \"AvgAHH\", \"AvgAHA\", \"Unnamed: 0\", \"Date\", \"HTHG\", \"FTR\", \"HTHG\", \"HTAG\", \"HTR\", \"Index\", \"country\",\n",
    "    \"PSCH\", \"PSCD\", \"PSCA\", \"Time\", \"B365CH\", \"B365CD\", \"B365CA\",\n",
    "    \"BWCH\", \"BWCD\", \"BWCA\", \"IWCH\", \"IWCD\", \"IWCA\", \"WHCH\", \"WHCD\",\n",
    "    \"WHCA\", \"VCCH\", \"VCCD\", \"VCCA\", \"MaxCH\", \"MaxCD\", \"MaxCA\", \"AvgCH\",\n",
    "    \"AvgCD\", \"AvgCA\", \"B365C>2.5\", \"B365C<2.5\", \"PC>2.5\", \"PC<2.5\",\n",
    "    \"MaxC>2.5\", \"MaxC<2.5\", \"AvgC>2.5\", \"AvgC<2.5\", \"AHCh\", \"B365CAHH\",\n",
    "    \"B365CAHA\", \"PCAHH\", \"PCAHA\", \"MaxCAHH\", \"MaxCAHA\", \"AvgCAHH\", \"AvgCAHA\", \"Attendance\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc7351b4b5b40e52"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done df_belgium\n",
      "done df_england\n",
      "done df_france\n",
      "done df_germany\n",
      "done df_greece\n",
      "done df_italy\n",
      "done df_netherlands\n",
      "done df_portugal\n",
      "done df_scotland\n",
      "done df_spain\n",
      "done df_turkey\n"
     ]
    }
   ],
   "source": [
    "for country in dfs:\n",
    "    for col in combined_labels:\n",
    "        dfs[country].drop(col,axis=1, inplace=True, errors='ignore')\n",
    "    dfs[country][\"Target_regr\"] = dfs[country][\"FTHG\"] + dfs[country][\"FTAG\"]\n",
    "    dfs[country]['Target_clas'] = [0 if a > h else 1 if h > a else -1 for a, h in zip(dfs[country]['FTAG'], dfs[country]['FTHG'])]\n",
    "    dfs[country] = dfs[country][dfs[country][\"season\"] != 0]\n",
    "    dfs[country] = pd.get_dummies(df['Avg_bookie_prediction'], prefix='Bookie_Prediction')\n",
    "\n",
    "    print(f\"done {country}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T21:27:16.688039300Z",
     "start_time": "2024-01-04T21:27:10.989956500Z"
    }
   },
   "id": "c8f894d197f23716"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "   Bookie_Prediction_A  Bookie_Prediction_D  Bookie_Prediction_H\n0                 True                False                False\n1                 True                False                False\n2                 True                False                False\n3                False                False                 True\n4                False                False                 True",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bookie_Prediction_A</th>\n      <th>Bookie_Prediction_D</th>\n      <th>Bookie_Prediction_H</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[country].head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T21:27:52.399178400Z",
     "start_time": "2024-01-04T21:27:52.379127Z"
    }
   },
   "id": "a421cdbe5ddf60dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export Dataset as .csv file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ca8c6e9e8aa759e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d169ef4151ac073d"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'season'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32mC:\\SDKs\\ml_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3789\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3790\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3791\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'season'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[101], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m country \u001B[38;5;129;01min\u001B[39;00m dfs:\n\u001B[1;32m----> 2\u001B[0m     dfs[country][\u001B[43mdfs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcountry\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseason\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m22\u001B[39m]\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/train_preprocessed/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcountry\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m     dfs[country][dfs[country][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseason\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m22\u001B[39m]\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/test_preprocessed/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcountry\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\SDKs\\ml_venv\\lib\\site-packages\\pandas\\core\\frame.py:3896\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3895\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3896\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3898\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32mC:\\SDKs\\ml_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3793\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3794\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3795\u001B[0m     ):\n\u001B[0;32m   3796\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3797\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3798\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3799\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3800\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3801\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'season'"
     ]
    }
   ],
   "source": [
    "for country in dfs:\n",
    "    dfs[country][dfs[country][\"season\"] != 22].to_csv(f\"data/train_preprocessed/{country}.csv\")\n",
    "    dfs[country][dfs[country][\"season\"] == 22].to_csv(f\"data/test_preprocessed/{country}.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T21:27:27.458095900Z",
     "start_time": "2024-01-04T21:27:27.401281Z"
    }
   },
   "id": "7515491c70f17e4c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
