{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:01:09.387238900Z",
     "start_time": "2024-01-07T17:01:09.235316Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,CategoricalNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6491863b8ff047b2"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "dfs_train = {}\n",
    "dfs_test = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:01:09.506889700Z",
     "start_time": "2024-01-07T17:01:09.246374200Z"
    }
   },
   "id": "67e99dc3b60e9c20"
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    }
   ],
   "source": [
    "for root, directory, files in os.walk(\"data/train_preprocessed\", topdown=False):\n",
    "    if files:\n",
    "        for file in files:\n",
    "            dfs_train[file[:-4]] = pd.read_csv(f\"{root}/{file}\")\n",
    "            try:\n",
    "                dfs_train[file[:-4]] = pd.get_dummies(dfs_train[file[:-4]], columns=[\"Avg_bookie_prediction\"], prefix='Bookie_Prediction')\n",
    "            except KeyError:\n",
    "                pass\n",
    "lens_test = 0\n",
    "for root, directory, files in os.walk(\"data/test_preprocessed\", topdown=False):\n",
    "    if files:\n",
    "        for file in files:\n",
    "            dfs_test[file[:-4]] = pd.read_csv(f\"{root}/{file}\")\n",
    "    try:\n",
    "        dfs_test[file[:-4]] = pd.get_dummies(dfs_test[file[:-4]], columns=[\"Avg_bookie_prediction\"], prefix='Bookie_Prediction')\n",
    "    except KeyError:\n",
    "        pass\n",
    "            # lens_test += dfs_test[file[:-4]].shape[0]\n",
    "print(\"---\")\n",
    "lens_orig = 0\n",
    "df_test_y = pd.DataFrame()\n",
    "for root, directory, files in os.walk(\"data/orig_data\", topdown=False):\n",
    "    if files:\n",
    "        for file in files:\n",
    "            # print(pd.read_csv(f\"{root}/{file}\").shape)\n",
    "            tmp = pd.read_csv(f\"{root}/{file}\")\n",
    "            lens_orig += tmp.shape[0]\n",
    "            tmp[\"country\"] = file[:-5]\n",
    "            \n",
    "            df_test_y = pd.concat([df_test_y, tmp], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:08:52.982471200Z",
     "start_time": "2024-01-07T18:08:49.158833800Z"
    }
   },
   "id": "c7af8ff6f8ea0ba"
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "0       True\n1      False\n2      False\n3       True\n4       True\n       ...  \n337    False\n338    False\n339    False\n340    False\n341    False\nName: Bookie_Prediction_A, Length: 342, dtype: bool"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:08:22.867861Z",
     "start_time": "2024-01-07T18:08:22.850574Z"
    }
   },
   "id": "7482cfca1a53487a"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "df_test_y[\"Date\"] = pd.to_datetime(df_test_y[\"Date\"], format = '%d/%m/%Y', dayfirst=True)\n",
    "\n",
    "df_test_y.sort_values(by=\"Date\", inplace=True, ascending=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:01:13.571911100Z",
     "start_time": "2024-01-07T17:01:13.529008Z"
    }
   },
   "id": "ff5e531d024f1fd8"
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'league'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32mC:\\SDKs\\ml_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3789\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3790\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3791\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'league'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[178], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdf_test_y\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdf_test_y\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcountry\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mengland\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mleague\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32mC:\\SDKs\\ml_venv\\lib\\site-packages\\pandas\\core\\frame.py:3896\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3895\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3896\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3898\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32mC:\\SDKs\\ml_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3793\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3794\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3795\u001B[0m     ):\n\u001B[0;32m   3796\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3797\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3798\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3799\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3800\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3801\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'league'"
     ]
    }
   ],
   "source": [
    "df_test_y[df_test_y[\"country\"]==\"england\"][\"league\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:12:00.405146Z",
     "start_time": "2024-01-07T18:12:00.348279600Z"
    }
   },
   "id": "afbab6d2afef5660"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "for country in df_test_y[\"country\"].unique():\n",
    "    col1 = df_test_y[df_test_y[\"country\"] == country][\"FTHG\"].reset_index()\n",
    "    col2 = df_test_y[df_test_y[\"country\"] == country][\"FTAG\"].reset_index()\n",
    "\n",
    "    target_values = col1[\"FTHG\"] + col2[\"FTAG\"]\n",
    "\n",
    "    dfs_test[f\"df_{country}\"][\"Target_regr\"] = target_values\n",
    "    dfs_test[f\"df_{country}\"][\"FTHG\"] = col1[\"FTHG\"]\n",
    "    dfs_test[f\"df_{country}\"][\"FTAG\"] = col2[\"FTAG\"]\n",
    "    dfs_test[f\"df_{country}\"]['Target_clas'] = [0 if a > h else 1 if h > a else -1 for a, h in zip(dfs_test[f\"df_{country}\"]['FTAG'], dfs_test[f\"df_{country}\"]['FTHG'])]\n",
    "    dfs_test[f\"df_{country}\"].drop(columns=[\"FTHG\", \"FTAG\", \"Unnamed: 0\"], inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:01:13.675935600Z",
     "start_time": "2024-01-07T17:01:13.576024300Z"
    }
   },
   "id": "85233e29ca135574"
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'div'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32mC:\\SDKs\\ml_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3789\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3790\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3791\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'div'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[177], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdfs_train\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdf_england\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdiv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32mC:\\SDKs\\ml_venv\\lib\\site-packages\\pandas\\core\\frame.py:3896\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3895\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3896\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3898\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32mC:\\SDKs\\ml_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3793\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3794\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3795\u001B[0m     ):\n\u001B[0;32m   3796\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3797\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3798\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3799\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3800\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3801\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'div'"
     ]
    }
   ],
   "source": [
    "dfs_train[\"df_england\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:11:02.309947400Z",
     "start_time": "2024-01-07T18:11:02.256747700Z"
    }
   },
   "id": "309339a0458df882"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def impute_nan_values(dfs):\n",
    "    for df in dfs.values():\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == np.float64 or df[col].dtype == np.int64:\n",
    "                df[col] = df.groupby(\"season\")[col].transform(lambda x: x.fillna(x.mean()))\n",
    "        df.dropna(inplace=True)\n",
    "impute_nan_values(dfs_train)\n",
    "impute_nan_values(dfs_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:01:15.183929100Z",
     "start_time": "2024-01-07T17:01:13.669416100Z"
    }
   },
   "id": "6a00c509f5e49cb3"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vojta\\AppData\\Local\\Temp\\ipykernel_27868\\27335406.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfs_train[country]['Target_clas'] = [0 if a > h else 1 if h > a else -1 for a, h in zip(dfs_train[country]['FTAG'], dfs_train[country]['FTHG'])]\n"
     ]
    }
   ],
   "source": [
    "# validation set\n",
    "dfs_valid_reg_X = {}\n",
    "dfs_valid_reg_y = {}\n",
    "dfs_train_reg_X = {}\n",
    "dfs_train_reg_y = {}\n",
    "dfs_valid_clas_X = {}\n",
    "dfs_valid_clas_y = {}\n",
    "dfs_train_clas_X = {}\n",
    "dfs_train_clas_y = {}\n",
    "dfs_test_clas_X = {}\n",
    "dfs_test_clas_y = {}\n",
    "dfs_test_reg_X = {}\n",
    "dfs_test_reg_y = {}\n",
    "\n",
    "cols_to_drop = ['FTHG', 'FTAG', 'MatchTeams', 'SameHomeTeam', 'Target', 'Target_regr', 'Target_clas', \"Unnamed: 0\", \"index\", \"season\"]\n",
    "\n",
    "for country in dfs_train:\n",
    "    dfs_train[country] =   dfs_train[country][dfs_train[country][\"season\"] > 17]\n",
    "    dfs_train[country]['Target_clas'] = [0 if a > h else 1 if h > a else -1 for a, h in zip(dfs_train[country]['FTAG'], dfs_train[country]['FTHG'])]\n",
    "\n",
    "    dfs_valid_reg_y[country[3:]] = dfs_train[country][dfs_train[country][\"season\"] == 21][\"Target_regr\"]\n",
    "    dfs_valid_reg_X[country[3:]] = dfs_train[country][dfs_train[country][\"season\"] == 21].drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    dfs_train_reg_y[country[3:]] = dfs_train[country][dfs_train[country][\"season\"] != 21][\"Target_regr\"]\n",
    "    dfs_train_reg_X[country[3:]] = dfs_train[country][dfs_train[country][\"season\"] != 21].drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    dfs_train_clas_y[country[3:]] = dfs_train[country][dfs_train[country][\"season\"] != 21][\"Target_clas\"]\n",
    "    dfs_train_clas_X[country[3:]] = dfs_train[country][dfs_train[country][\"season\"] != 21].drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    dfs_valid_clas_y[country[3:]] = dfs_train[country][dfs_train[country][\"season\"] == 21][\"Target_clas\"]\n",
    "    dfs_valid_clas_X[country[3:]] = dfs_train[country][dfs_train[country][\"season\"] == 21].drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "for country in dfs_test:\n",
    "    dfs_test_reg_y[country[3:]] = dfs_test[country][dfs_test[country][\"season\"] != 21][\"Target_regr\"]\n",
    "    dfs_test_reg_X[country[3:]] = dfs_test[country][dfs_test[country][\"season\"] != 21].drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    dfs_test_clas_y[country[3:]] = dfs_test[country][dfs_test[country][\"season\"] != 21][\"Target_clas\"]\n",
    "    dfs_test_clas_X[country[3:]] = dfs_test[country][dfs_test[country][\"season\"] != 21].drop(columns=cols_to_drop, errors='ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:01:15.352576500Z",
     "start_time": "2024-01-07T17:01:15.188203300Z"
    }
   },
   "id": "bc85102fe8888b5a"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "train_results = pd.DataFrame(columns=[\"country\"])\n",
    "test_results = pd.DataFrame(columns=[\"country\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:01:15.396588500Z",
     "start_time": "2024-01-07T17:01:15.353578900Z"
    }
   },
   "id": "adc91403dbf185be"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "train_results[\"country\"] = dfs_train_clas_X.keys()\n",
    "test_results[\"country\"] = dfs_train_clas_X.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:01:15.404665400Z",
     "start_time": "2024-01-07T17:01:15.368032500Z"
    }
   },
   "id": "4fbe116bc1af055e"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "        country\n0       belgium\n1       england\n2        france\n3       germany\n4        greece\n5         italy\n6   netherlands\n7      portugal\n8      scotland\n9         spain\n10       turkey",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>belgium</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>england</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>france</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>germany</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>greece</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>italy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>netherlands</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>portugal</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>scotland</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>spain</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>turkey</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:01:15.406855200Z",
     "start_time": "2024-01-07T17:01:15.384067100Z"
    }
   },
   "id": "9d6866e75a8203e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification task\n",
    "We decided to go with Voting Classifier consisting of 3 classification algorithms -  Gaussian Naive Bayes, RandomForestClassifier & Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "382011daf10761e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71023eb496babff"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance on validation data for belgium is 0.4463821716453295 for C = 1 and penalty = l2\n",
      "Best performance on validation data for england is 0.4054037836997059 for C = 10 and penalty = l2\n",
      "Best performance on validation data for france is 0.4075106206252051 for C = 1 and penalty = l1\n",
      "Best performance on validation data for germany is 0.41897415919793746 for C = 100 and penalty = l2\n",
      "Best performance on validation data for greece is 0.43259113403698635 for C = 0.1 and penalty = l2\n",
      "Best performance on validation data for italy is 0.43277442981349384 for C = 1 and penalty = l2\n",
      "Best performance on validation data for netherlands is 0.4136953877901773 for C = 1 and penalty = l1\n",
      "Best performance on validation data for portugal is 0.45984829350235934 for C = 10 and penalty = l1\n",
      "Best performance on validation data for scotland is 0.4195134685220309 for C = 1 and penalty = l2\n",
      "Best performance on validation data for spain is 0.42241395767095186 for C = 1 and penalty = l1\n",
      "Best performance on validation data for turkey is 0.4017914251021839 for C = 0.1 and penalty = l2\n"
     ]
    }
   ],
   "source": [
    "dfs_test_predict = {}\n",
    "dfs_train_predict = {}\n",
    "dfs_valid_predict = {}\n",
    "dfs_best_params_lr = {}\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]  # Example values for regularization parameter C\n",
    "penalty_values = ['l1', 'l2']  # Example values for penalty (regularization term)\n",
    "\n",
    "for country in dfs_train_clas_X.keys():\n",
    "    max_value = 0\n",
    "    result = [[0 for _ in range(len(penalty_values))] for _ in range(len(C_values))]\n",
    "    max_i = -1\n",
    "    max_j = -1\n",
    "\n",
    "    for i, C in enumerate(C_values):\n",
    "        for j, penalty in enumerate(penalty_values):\n",
    "            lr = LogisticRegression(C=C, penalty=penalty, solver='liblinear', max_iter=10000)\n",
    "            lr.fit(dfs_train_clas_X[country], dfs_train_clas_y[country])\n",
    "            dfs_valid_predict[country] = lr.predict(dfs_valid_clas_X[country])\n",
    "            result[i][j] = f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')\n",
    "\n",
    "    for i in range(len(C_values)):\n",
    "        for j in range(len(penalty_values)):\n",
    "            current_value = result[i][j]\n",
    "            if current_value > max_value:\n",
    "                max_value = current_value\n",
    "                max_i = i\n",
    "                max_j = j\n",
    "\n",
    "    best_C = C_values[max_i]\n",
    "    best_penalty = penalty_values[max_j]\n",
    "    dfs_best_params_lr[country] = {\"C\": best_C, \"penalty\": best_penalty}\n",
    "    test_results.loc[train_results[\"country\"] == country, \"lr\"] = max_value\n",
    "    print(f\"Best performance on validation data for {country} is {max_value} for C = {best_C} and penalty = {best_penalty}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:02:37.949484300Z",
     "start_time": "2024-01-07T17:01:15.401593700Z"
    }
   },
   "id": "4ff2d034a0f25a24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest Model\n",
    "\n",
    "Utilization of basic grid search to find best parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "594858abcbcb887e"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "rfm = RandomForestClassifier(random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:02:37.966656100Z",
     "start_time": "2024-01-07T17:02:37.949484300Z"
    }
   },
   "id": "c6f8b8ba6737e767"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "train_results_random_forest = {}\n",
    "def random_forest(params):\n",
    "    rfm.set_params(**params)\n",
    "    rfm.fit(dfs_train_clas_X[country], dfs_train_clas_y[country])\n",
    "    dfs_train_predict[country] = rfm.predict(dfs_train_clas_X[country])\n",
    "    dfs_valid_predict[country] = rfm.predict(dfs_valid_clas_X[country])\n",
    "    dfs_test_predict[country] = rfm.predict(dfs_test_clas_X[country])\n",
    "        \n",
    "    # print(f\"Average Depth of Decision Trees for {country}: {average_depth}\")\n",
    "    train_results[params[\"max_depth\"]] = f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')\n",
    "    result_train = f1_score(dfs_train_clas_y[country], dfs_train_predict[country], average='macro')\n",
    "    result_valid = f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')\n",
    "    result_test =  f1_score(dfs_test_clas_y[country], dfs_test_predict[country], average='macro')\n",
    "    return result_valid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:02:37.991695900Z",
     "start_time": "2024-01-07T17:02:37.965653400Z"
    }
   },
   "id": "4359c844e840482b"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance on validation data for belgium is 0.45683323496622735 for max_depth = 23 and min_samples_split = 2\n",
      "Best performance on validation data for england is 0.389815488249824 for max_depth = 21 and min_samples_split = 2\n",
      "Best performance on validation data for france is 0.42556743721259843 for max_depth = 17 and min_samples_split = 15\n",
      "Best performance on validation data for germany is 0.39388206484610616 for max_depth = 23 and min_samples_split = 2\n",
      "Best performance on validation data for greece is 0.4526915050241622 for max_depth = 16 and min_samples_split = 5\n",
      "Best performance on validation data for italy is 0.44078503048354717 for max_depth = 15 and min_samples_split = 2\n",
      "Best performance on validation data for netherlands is 0.47335343509641453 for max_depth = 10 and min_samples_split = 2\n",
      "Best performance on validation data for portugal is 0.4809563576017399 for max_depth = 19 and min_samples_split = 5\n",
      "Best performance on validation data for scotland is 0.41141796875435377 for max_depth = 23 and min_samples_split = 2\n",
      "Best performance on validation data for spain is 0.4508945591071461 for max_depth = 19 and min_samples_split = 20\n",
      "Best performance on validation data for turkey is 0.42406340766996503 for max_depth = 20 and min_samples_split = 2\n"
     ]
    }
   ],
   "source": [
    "dfs_train_predict = {}\n",
    "dfs_valid_predict = {}\n",
    "dfs_best_params_rfm = {}\n",
    "max_depth_values = [i for i in range(10, 25)]\n",
    "min_samples_split_values = [2, 5, 15, 10, 20]\n",
    "\n",
    "for country in dfs_train_clas_X.keys():\n",
    "    max_value = 0\n",
    "    result = [[0 for _ in range(len(min_samples_split_values))] for _ in range(len(max_depth_values))]\n",
    "    max_i = -1\n",
    "    max_j = -1\n",
    "\n",
    "    for i, max_depth in enumerate(max_depth_values):\n",
    "        for j, min_samples_split in enumerate(min_samples_split_values):\n",
    "            result[i][j] = random_forest({\"max_depth\": max_depth + 1, \"min_samples_split\": min_samples_split})\n",
    "\n",
    "    for i in range(len(max_depth_values)):\n",
    "        for j in range(len(min_samples_split_values)):\n",
    "            current_value = result[i][j]\n",
    "            if current_value > max_value:\n",
    "                max_value = current_value\n",
    "                max_i = i\n",
    "                max_j = j\n",
    "\n",
    "    best_max_depth = max_depth_values[max_i]\n",
    "    best_min_samples_split = min_samples_split_values[max_j]\n",
    "    test_results.loc[test_results[\"country\"] == country, \"rfm\"] = max_value\n",
    "    dfs_best_params_rfm[country] = {\"max_depth\": best_max_depth, \"min_samples_split\": best_min_samples_split}\n",
    "\n",
    "    print(f\"Best performance on validation data for {country} is {max_value} for max_depth = {best_max_depth} and min_samples_split = {best_min_samples_split}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:09:02.219754600Z",
     "start_time": "2024-01-07T17:02:37.982174200Z"
    }
   },
   "id": "5eda447e36f2dcef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MultinomialNaive Bayes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab31cc39a53d29b"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:09:02.231874100Z",
     "start_time": "2024-01-07T17:09:02.216752400Z"
    }
   },
   "id": "561c2f796f6b828a"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for baseline Multinomial Naive Bayes model on train data for belgium: 0.4255711292491478\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for belgium: 0.40595500070221185\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for belgium: 0.3616061357996842\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for england: 0.47621455325225986\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for england: 0.44302221161866573\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for england: 0.3425640810954196\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for france: 0.48709385058962384\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for france: 0.44166874510691706\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for france: 0.38679821408619536\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for germany: 0.46336642551790447\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for germany: 0.413038350868521\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for germany: 0.3887743081341389\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for greece: 0.5469379241427905\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for greece: 0.4771362271362271\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for greece: 0.4371217243428196\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for italy: 0.5090069527518928\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for italy: 0.45200712455026654\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for italy: 0.3290309760897996\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for netherlands: 0.46890382580073275\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for netherlands: 0.45944954874502475\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for netherlands: 0.37420909062073776\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for portugal: 0.508725820834707\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for portugal: 0.5501816191471364\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for portugal: 0.39751005515271937\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for scotland: 0.4889821013453819\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for scotland: 0.4592283837507023\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for scotland: 0.37222841983952354\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for spain: 0.4831325349594879\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for spain: 0.4409106395416836\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for spain: 0.323316985113257\n",
      " --------\n",
      "F1-score for baseline Multinomial Naive Bayes model on train data for turkey: 0.4566879351403201\n",
      "F1-score for baseline Multinomial Naive Bayes model on validation data for turkey: 0.4162249678901572\n",
      "F1-score for baseline Multinomial Naive Bayes model on test data for turkey: 0.3517916547856667\n",
      " --------\n"
     ]
    }
   ],
   "source": [
    "dfs_train_predict = {}\n",
    "dfs_valid_predict = {}\n",
    "for country in dfs_train_clas_X.keys():\n",
    "    nb.fit(dfs_train_clas_X[country], dfs_train_clas_y[country])\n",
    "    dfs_train_predict[country] = nb.predict(dfs_train_clas_X[country])\n",
    "    dfs_valid_predict[country] = nb.predict(dfs_valid_clas_X[country])\n",
    "    dfs_test_predict[country] = nb.predict(dfs_test_clas_X[country])\n",
    "\n",
    "    test_results.loc[test_results[\"country\"] == country, \"nb\"] = f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')\n",
    "    # test_results.loc[test_results[\"country\"] == country, \"nb\"] = f1_score(dfs_test_clas_y[country], dfs_test_predict[country], average='macro')\n",
    "\n",
    "    print(f\"F1-score for baseline Multinomial Naive Bayes model on train data for {country}: {f1_score(dfs_train_clas_y[country], dfs_train_predict[country], average='macro')}\")\n",
    "    print(f\"F1-score for baseline Multinomial Naive Bayes model on validation data for {country}: {f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')}\")\n",
    "    print(f\"F1-score for baseline Multinomial Naive Bayes model on test data for {country}: {f1_score(dfs_test_clas_y[country], dfs_test_predict[country], average='macro')}\\n --------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:09:02.496741600Z",
     "start_time": "2024-01-07T17:09:02.233376400Z"
    }
   },
   "id": "35ee17d02a2f0cc5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNeighborsClassifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e3b120342bd53c8"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "kn = KNeighborsClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:09:02.509485600Z",
     "start_time": "2024-01-07T17:09:02.465179700Z"
    }
   },
   "id": "21b749850dc349dc"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for baseline KNeighbors model on train data for belgium: 0.5710600107944851\n",
      "F1-score for baseline KNeighbors model on validation data for belgium: 0.46310744352754857\n",
      "F1-score for baseline KNeighbors model on test data for belgium: 0.3701209330737993\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for england: 0.5827749869996115\n",
      "F1-score for baseline KNeighbors model on validation data for england: 0.3933019717107375\n",
      "F1-score for baseline KNeighbors model on test data for england: 0.31704960062302684\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for france: 0.6045559302159741\n",
      "F1-score for baseline KNeighbors model on validation data for france: 0.4070633684926858\n",
      "F1-score for baseline KNeighbors model on test data for france: 0.3567566253218599\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for germany: 0.5979028682142488\n",
      "F1-score for baseline KNeighbors model on validation data for germany: 0.43189945723011075\n",
      "F1-score for baseline KNeighbors model on test data for germany: 0.384369081377897\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for greece: 0.6100450968042084\n",
      "F1-score for baseline KNeighbors model on validation data for greece: 0.43349309420737986\n",
      "F1-score for baseline KNeighbors model on test data for greece: 0.4091988858064483\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for italy: 0.6109617314835425\n",
      "F1-score for baseline KNeighbors model on validation data for italy: 0.4406765197891998\n",
      "F1-score for baseline KNeighbors model on test data for italy: 0.34909135283549286\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for netherlands: 0.6043986490010157\n",
      "F1-score for baseline KNeighbors model on validation data for netherlands: 0.4385340755913578\n",
      "F1-score for baseline KNeighbors model on test data for netherlands: 0.36545163818899234\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for portugal: 0.6177226470120095\n",
      "F1-score for baseline KNeighbors model on validation data for portugal: 0.5287392482274752\n",
      "F1-score for baseline KNeighbors model on test data for portugal: 0.36460628684321233\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for scotland: 0.6279892871096441\n",
      "F1-score for baseline KNeighbors model on validation data for scotland: 0.4144833697072503\n",
      "F1-score for baseline KNeighbors model on test data for scotland: 0.33775808208797903\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for spain: 0.5783881318696844\n",
      "F1-score for baseline KNeighbors model on validation data for spain: 0.4248594670050399\n",
      "F1-score for baseline KNeighbors model on test data for spain: 0.3431502479472022\n",
      " --------\n",
      "F1-score for baseline KNeighbors model on train data for turkey: 0.5909003584385879\n",
      "F1-score for baseline KNeighbors model on validation data for turkey: 0.3890698194260256\n",
      "F1-score for baseline KNeighbors model on test data for turkey: 0.3555659155659156\n",
      " --------\n"
     ]
    }
   ],
   "source": [
    "dfs_train_predict = {}\n",
    "dfs_valid_predict = {}\n",
    "for country in dfs_train_clas_X.keys():\n",
    "    kn.fit(dfs_train_clas_X[country], dfs_train_clas_y[country])\n",
    "    dfs_train_predict[country] = kn.predict(dfs_train_clas_X[country])\n",
    "    dfs_valid_predict[country] = kn.predict(dfs_valid_clas_X[country])\n",
    "    dfs_test_predict[country] = kn.predict(dfs_test_clas_X[country])\n",
    "    test_results.loc[test_results[\"country\"] == country, \"kn\"] = f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')\n",
    "    # test_results.loc[test_results[\"country\"] == country, \"kn\"] = f1_score(dfs_test_clas_y[country], dfs_test_predict[country], average='macro')\n",
    "\n",
    "    print(f\"F1-score for baseline KNeighbors model on train data for {country}: {f1_score(dfs_train_clas_y[country], dfs_train_predict[country], average='macro')}\")\n",
    "    print(f\"F1-score for baseline KNeighbors model on validation data for {country}: {f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')}\")\n",
    "    print(f\"F1-score for baseline KNeighbors model on test data for {country}: {f1_score(dfs_test_clas_y[country], dfs_test_predict[country], average='macro')}\\n --------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:09:04.512570200Z",
     "start_time": "2024-01-07T17:09:02.465179700Z"
    }
   },
   "id": "cfd82a05eab38d7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Boosting Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1155353c873345e8"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:09:04.527713100Z",
     "start_time": "2024-01-07T17:09:04.510500100Z"
    }
   },
   "id": "ca95c293c0f6ad91"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for baseline GradientBoostingClassifier model on train data for belgium: 0.8681219669740553\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for belgium: 0.42784629583018713\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for belgium: 0.34212471619409685\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for england: 0.5295075625406082\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for england: 0.39290681987422227\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for england: 0.3035912952227761\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for france: 0.7090153120670831\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for france: 0.37029676863346017\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for france: 0.3393761903526264\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for germany: 0.7031926030103174\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for germany: 0.3788707751080728\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for germany: 0.3606677099634846\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for greece: 0.9205465717660841\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for greece: 0.39290580820984505\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for greece: 0.42670766936911747\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for italy: 0.7028600221700362\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for italy: 0.458734435366694\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for italy: 0.3252662479955926\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for netherlands: 0.8643109206421209\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for netherlands: 0.4118182165653786\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for netherlands: 0.3711332836619137\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for portugal: 0.8951653602206004\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for portugal: 0.46412002316418066\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for portugal: 0.34844810493403494\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for scotland: 0.7188447164686919\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for scotland: 0.4161448787422309\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for scotland: 0.3350032955784575\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for spain: 0.6653139439566722\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for spain: 0.42622638440557514\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for spain: 0.32603321092642695\n",
      " --------\n",
      "F1-score for baseline GradientBoostingClassifier model on train data for turkey: 0.8533336791474243\n",
      "F1-score for baseline GradientBoostingClassifier model on validation data for turkey: 0.37218595896856765\n",
      "F1-score for baseline GradientBoostingClassifier model on test data for turkey: 0.34116567956097815\n",
      " --------\n"
     ]
    }
   ],
   "source": [
    "dfs_train_predict = {}\n",
    "dfs_valid_predict = {}\n",
    "for country in dfs_train_clas_X.keys():\n",
    "    gbc.fit(dfs_train_clas_X[country], dfs_train_clas_y[country])\n",
    "    dfs_train_predict[country] = gbc.predict(dfs_train_clas_X[country])\n",
    "    dfs_valid_predict[country] = gbc.predict(dfs_valid_clas_X[country])\n",
    "    dfs_test_predict[country] = gbc.predict(dfs_test_clas_X[country])\n",
    "\n",
    "    test_results.loc[test_results[\"country\"] == country, \"gbc\"] = f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')\n",
    "    # test_results.loc[test_results[\"country\"] == country, \"gbc\"] = f1_score(dfs_test_clas_y[country], dfs_test_predict[country], average='macro')\n",
    "    print(f\"F1-score for baseline GradientBoostingClassifier model on train data for {country}: {f1_score(dfs_train_clas_y[country], dfs_train_predict[country], average='macro')}\")\n",
    "    print(f\"F1-score for baseline GradientBoostingClassifier model on validation data for {country}: {f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')}\")\n",
    "    print(f\"F1-score for baseline GradientBoostingClassifier model on test data for {country}: {f1_score(dfs_test_clas_y[country], dfs_test_predict[country], average='macro')}\\n --------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:10:04.703369Z",
     "start_time": "2024-01-07T17:09:04.527713100Z"
    }
   },
   "id": "4cc3cc37f905e8df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ada Boosting Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9dc950798a686f6"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for baseline AdaBoostClassifier model on train data for belgium: 0.5693628535338963\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for belgium: 0.41463340555206835\n",
      "F1-score for baseline AdaBoostClassifier model on test data for belgium: 0.34059320474975524\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for england: 0.4230871646336721\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for england: 0.36791561566605385\n",
      "F1-score for baseline AdaBoostClassifier model on test data for england: 0.3077086469035622\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for france: 0.510443312034176\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for france: 0.3852251491921093\n",
      "F1-score for baseline AdaBoostClassifier model on test data for france: 0.33881003634190376\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for germany: 0.4975428032560056\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for germany: 0.3976329295618332\n",
      "F1-score for baseline AdaBoostClassifier model on test data for germany: 0.33186844621709316\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for greece: 0.6134324466297653\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for greece: 0.42178997292504405\n",
      "F1-score for baseline AdaBoostClassifier model on test data for greece: 0.42504135584756075\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for italy: 0.4962381196747718\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for italy: 0.4377796044115363\n",
      "F1-score for baseline AdaBoostClassifier model on test data for italy: 0.3430593395312574\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for netherlands: 0.5836129132937375\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for netherlands: 0.43111552746005904\n",
      "F1-score for baseline AdaBoostClassifier model on test data for netherlands: 0.34110381707865933\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for portugal: 0.5776280582140375\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for portugal: 0.46437772858636167\n",
      "F1-score for baseline AdaBoostClassifier model on test data for portugal: 0.3691351445036613\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for scotland: 0.49003578286809013\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for scotland: 0.38910035976847485\n",
      "F1-score for baseline AdaBoostClassifier model on test data for scotland: 0.34606553641197424\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for spain: 0.47248747625531634\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for spain: 0.3971640124365128\n",
      "F1-score for baseline AdaBoostClassifier model on test data for spain: 0.3130309087152654\n",
      " --------\n",
      "F1-score for baseline AdaBoostClassifier model on train data for turkey: 0.5410136923283325\n",
      "F1-score for baseline AdaBoostClassifier model on validation data for turkey: 0.38849374495560934\n",
      "F1-score for baseline AdaBoostClassifier model on test data for turkey: 0.375132861230895\n",
      " --------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "dfs_train_predict = {}\n",
    "dfs_valid_predict = {}\n",
    "dfs_test_predict = {}\n",
    "\n",
    "for country in dfs_train_clas_X.keys():\n",
    "    # Create an AdaBoostClassifier\n",
    "    abc = AdaBoostClassifier(n_estimators=50, random_state=42)  # You can adjust the hyperparameters as needed\n",
    "\n",
    "    # Train the AdaBoostClassifier\n",
    "    abc.fit(dfs_train_clas_X[country], dfs_train_clas_y[country])\n",
    "\n",
    "    # Make predictions on train, validation, and test sets\n",
    "    dfs_train_predict[country] = abc.predict(dfs_train_clas_X[country])\n",
    "    dfs_valid_predict[country] = abc.predict(dfs_valid_clas_X[country])\n",
    "    dfs_test_predict[country] = abc.predict(dfs_test_clas_X[country])\n",
    "\n",
    "    # Calculate and store F1-score on validation set\n",
    "    test_results.loc[test_results[\"country\"] == country, \"abc\"] = f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')\n",
    "    # test_results.loc[test_results[\"country\"] == country, \"abc\"] = f1_score(dfs_test_clas_y[country], dfs_test_predict[country], average='macro')\n",
    "\n",
    "    # Print F1-scores\n",
    "    print(f\"F1-score for baseline AdaBoostClassifier model on train data for {country}: {f1_score(dfs_train_clas_y[country], dfs_train_predict[country], average='macro')}\")\n",
    "    print(f\"F1-score for baseline AdaBoostClassifier model on validation data for {country}: {f1_score(dfs_valid_clas_y[country], dfs_valid_predict[country], average='macro')}\")\n",
    "    print(f\"F1-score for baseline AdaBoostClassifier model on test data for {country}: {f1_score(dfs_test_clas_y[country], dfs_test_predict[country], average='macro')}\\n --------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:10:12.319850Z",
     "start_time": "2024-01-07T17:10:04.703369Z"
    }
   },
   "id": "8aa6d89b47e54141"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementing Voting Classifier\n",
    "Our voting classifier is designed to use the 3 best performing classifiers and use them for the final prediction "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed85c4ceff50c438"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "best_models = []\n",
    "for i, row in test_results.iterrows():\n",
    "    best_models.append(pd.to_numeric(row[[\"lr\", \"rfm\", \"nb\", \"kn\", \"gbc\", \"abc\"]]).nlargest(4).index.tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:24:21.939574200Z",
     "start_time": "2024-01-07T17:24:21.917672800Z"
    }
   },
   "id": "1143fe15b8757fbd"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "def voting_classifier(best_models_country, X_train, y_train, X_val, y_val, country: str, X_test, y_test):\n",
    "    \"\"\"\"\"\"\n",
    "    models = {\n",
    "        \"nb\": nb,\n",
    "        \"kn\": kn,\n",
    "        \"gbc\": GradientBoostingClassifier(random_state=42),\n",
    "        \"rfm\": RandomForestClassifier(random_state=42),\n",
    "        \"lr\": LogisticRegression(random_state=42, solver='liblinear', max_iter=10000),\n",
    "        \"abc\": AdaBoostClassifier()\n",
    "        \n",
    "    }\n",
    "    print(best_models_country)\n",
    "    clf1 = models[best_models_country[0]] if best_models_country[0] != \"rfm\" else models[best_models_country[0]].set_params(**dfs_best_params_rfm[country]) if best_models_country[0] != \"lr\" else models[best_models_country[0]].set_params(**dfs_best_params_lr[country])\n",
    "    clf2 = models[best_models_country[1]] if best_models_country[1] != \"rfm\" else models[best_models_country[1]].set_params(**dfs_best_params_rfm[country]) if best_models_country[0] != \"lr\" else models[best_models_country[0]].set_params(**dfs_best_params_lr[country])\n",
    "    clf3 = models[best_models_country[2]] if best_models_country[2] != \"rfm\" else models[best_models_country[2]].set_params(**dfs_best_params_rfm[country]) if best_models_country[0] != \"lr\" else models[best_models_country[0]].set_params(**dfs_best_params_lr[country])\n",
    "    \n",
    "    eclf = VotingClassifier(\n",
    "        estimators=[(best_models_country[0], clf1 ), (best_models_country[1], clf2 ),(best_models_country[2], clf3 )],\n",
    "        voting='soft', weights=[1.1,1,1]\n",
    "    ) \n",
    "    eclf.fit(X_train,y_train)\n",
    "    \n",
    "    y_train_predict = eclf.predict(X_train)\n",
    "    y_val_predict = eclf.predict(X_val)\n",
    "    \n",
    "    y_test_predict = eclf.predict(X_test)\n",
    "    clf1.fit(X_train, y_train)\n",
    "    y_valid_best_model = clf1.predict(X_val)\n",
    "    y_test_best_model = clf1.predict(X_test)\n",
    "    \n",
    "    print(f\"F1-score for voting classifier model on train data for {country}: {f1_score(y_train, y_train_predict, average='macro')}\")\n",
    "    print(f\"F1-score for voting classifier model on validation data for {country}: {f1_score(y_val, y_val_predict, average='macro')}\")\n",
    "    print(f\"F1-score for voting classifier model on test data for {country}: {f1_score(y_test, y_test_predict, average='macro')}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"F1-score for best model ({best_models_country[0]}) on valid data for {country}: {f1_score(y_val, y_valid_best_model, average='macro')}\")\n",
    "    print(f\"F1-score for best model ({best_models_country[0]}) on test data for {country}: {f1_score(y_test, y_test_best_model, average='macro')}\\n --------\")\n",
    "    X_test[\"final_prediction\"] =  y_test_predict if f1_score(y_val, y_val_predict, average='macro') > f1_score(y_val, y_valid_best_model, average='macro') else y_test_best_model\n",
    "    return X_test\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:09:31.872974Z",
     "start_time": "2024-01-07T18:09:31.860813700Z"
    }
   },
   "id": "fdb58c1352b112f8"
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kn', 'rfm', 'lr', 'gbc']\n",
      "F1-score for voting classifier model on train data for belgium: 0.8428295379519897\n",
      "F1-score for voting classifier model on validation data for belgium: 0.45599202441307707\n",
      "F1-score for voting classifier model on test data for belgium: 0.348206875678466\n",
      "\n",
      "\n",
      "F1-score for best model (kn) on valid data for belgium: 0.46310744352754857\n",
      "F1-score for best model (kn) on test data for belgium: 0.3701209330737993\n",
      " --------\n",
      "['nb', 'lr', 'kn', 'gbc']\n",
      "F1-score for voting classifier model on train data for england: 0.5385513871974877\n",
      "F1-score for voting classifier model on validation data for england: 0.4229649632551446\n",
      "F1-score for voting classifier model on test data for england: 0.325085105091345\n",
      "\n",
      "\n",
      "F1-score for best model (nb) on valid data for england: 0.44302221161866573\n",
      "F1-score for best model (nb) on test data for england: 0.3425640810954196\n",
      " --------\n",
      "['nb', 'rfm', 'lr', 'kn']\n",
      "F1-score for voting classifier model on train data for france: 0.5904066582374531\n",
      "F1-score for voting classifier model on validation data for france: 0.4239323250808075\n",
      "F1-score for voting classifier model on test data for france: 0.38929580217954357\n",
      "\n",
      "\n",
      "F1-score for best model (nb) on valid data for france: 0.44166874510691706\n",
      "F1-score for best model (nb) on test data for france: 0.38679821408619536\n",
      " --------\n",
      "['kn', 'lr', 'nb', 'abc']\n",
      "F1-score for voting classifier model on train data for germany: 0.5391280432884674\n",
      "F1-score for voting classifier model on validation data for germany: 0.41412880948845593\n",
      "F1-score for voting classifier model on test data for germany: 0.3658358269497985\n",
      "\n",
      "\n",
      "F1-score for best model (kn) on valid data for germany: 0.43189945723011075\n",
      "F1-score for best model (kn) on test data for germany: 0.384369081377897\n",
      " --------\n",
      "['nb', 'rfm', 'kn', 'lr']\n",
      "F1-score for voting classifier model on train data for greece: 0.7131434366279548\n",
      "F1-score for voting classifier model on validation data for greece: 0.46279060823414025\n",
      "F1-score for voting classifier model on test data for greece: 0.4264171016860752\n",
      "\n",
      "\n",
      "F1-score for best model (nb) on valid data for greece: 0.4771362271362271\n",
      "F1-score for best model (nb) on test data for greece: 0.4371217243428196\n",
      " --------\n",
      "['gbc', 'nb', 'rfm', 'kn']\n",
      "F1-score for voting classifier model on train data for italy: 0.7204000913052049\n",
      "F1-score for voting classifier model on validation data for italy: 0.4555827353657031\n",
      "F1-score for voting classifier model on test data for italy: 0.32607189882630566\n",
      "\n",
      "\n",
      "F1-score for best model (gbc) on valid data for italy: 0.458734435366694\n",
      "F1-score for best model (gbc) on test data for italy: 0.3252662479955926\n",
      " --------\n",
      "['rfm', 'nb', 'kn', 'abc']\n",
      "F1-score for voting classifier model on train data for netherlands: 0.7169045752420526\n",
      "F1-score for voting classifier model on validation data for netherlands: 0.48434112471649415\n",
      "F1-score for voting classifier model on test data for netherlands: 0.39275756646963683\n",
      "\n",
      "\n",
      "F1-score for best model (rfm) on valid data for netherlands: 0.45457342172537807\n",
      "F1-score for best model (rfm) on test data for netherlands: 0.3696094294253803\n",
      " --------\n",
      "['nb', 'kn', 'rfm', 'abc']\n",
      "F1-score for voting classifier model on train data for portugal: 0.7547489461193625\n",
      "F1-score for voting classifier model on validation data for portugal: 0.5410863646157763\n",
      "F1-score for voting classifier model on test data for portugal: 0.4474769174017295\n",
      "\n",
      "\n",
      "F1-score for best model (nb) on valid data for portugal: 0.5501816191471364\n",
      "F1-score for best model (nb) on test data for portugal: 0.39751005515271937\n",
      " --------\n",
      "['nb', 'lr', 'gbc', 'kn']\n",
      "F1-score for voting classifier model on train data for scotland: 0.5618470775489306\n",
      "F1-score for voting classifier model on validation data for scotland: 0.451453764808775\n",
      "F1-score for voting classifier model on test data for scotland: 0.36200798845504095\n",
      "\n",
      "\n",
      "F1-score for best model (nb) on valid data for scotland: 0.4592283837507023\n",
      "F1-score for best model (nb) on test data for scotland: 0.37222841983952354\n",
      " --------\n",
      "['rfm', 'nb', 'gbc', 'kn']\n",
      "F1-score for voting classifier model on train data for spain: 0.6309494404692546\n",
      "F1-score for voting classifier model on validation data for spain: 0.4330708411912602\n",
      "F1-score for voting classifier model on test data for spain: 0.31750134421240367\n",
      "\n",
      "\n",
      "F1-score for best model (rfm) on valid data for spain: 0.43989736875073326\n",
      "F1-score for best model (rfm) on test data for spain: 0.32006187740312314\n",
      " --------\n",
      "['rfm', 'nb', 'lr', 'kn']\n",
      "F1-score for voting classifier model on train data for turkey: 0.7847359763285319\n",
      "F1-score for voting classifier model on validation data for turkey: 0.4095661377607054\n",
      "F1-score for voting classifier model on test data for turkey: 0.35179988046747906\n",
      "\n",
      "\n",
      "F1-score for best model (rfm) on valid data for turkey: 0.39687416905486456\n",
      "F1-score for best model (rfm) on test data for turkey: 0.3534646326654523\n",
      " --------\n"
     ]
    }
   ],
   "source": [
    "final_predict = {}\n",
    "for i, country in enumerate(dfs_train_clas_X.keys()):\n",
    "    final_predict[country] = voting_classifier(best_models[i], dfs_train_clas_X[country], dfs_train_clas_y[country], dfs_valid_clas_X[country], dfs_valid_clas_y[country],country, dfs_test_clas_X[country], dfs_test_clas_y[country])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:10:05.172533600Z",
     "start_time": "2024-01-07T18:09:34.866693900Z"
    }
   },
   "id": "876472e32da6e2"
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "for country in final_predict:\n",
    "    final_predict[country] = np.where(final_predict[country] == 0, 'A', np.where(final_predict[country] == 1, 'H', 'D'))\n",
    "    df = pd.DataFrame(final_predict[country])\n",
    "    df.to_csv(f\"{country}.csv\", index= False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T17:52:15.658976300Z",
     "start_time": "2024-01-07T17:52:15.624919500Z"
    }
   },
   "id": "f65ead21d81d9a6a"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "      Avg_away_odds  Avg_home_odds  Avg_draw_odds  Var_away_odds  \\\n0          2.505000       2.908333       3.201667       0.000150   \n1          2.928333       2.413333       3.210000       0.015217   \n2          3.271667       2.228333       3.356667       0.007417   \n3          2.990000       2.456667       3.203333       0.004000   \n4          2.601667       2.631667       3.435000       0.002457   \n...             ...            ...            ...            ...   \n2031       2.343333       2.836667       3.523333       0.011307   \n2032       1.836667       3.806667       3.900000       0.002547   \n2033       3.350000       2.000000       3.746667       0.038000   \n2034       7.693333       1.361667       5.101667       0.135267   \n2035       1.411667       6.530000       4.965000       0.001897   \n\n      Var_home_odds  Var_draw_odds  LastMatchAwayGoals  LastMatchHomeGoals  \\\n0          0.007777       0.007217                 2.0                 1.0   \n1          0.002187       0.003600                 1.0                 1.0   \n2          0.001017       0.004267                 1.0                 2.0   \n3          0.003067       0.004067                 2.0                 1.0   \n4          0.000617       0.008150                 3.0                 1.0   \n...             ...            ...                 ...                 ...   \n2031       0.003787       0.012867                 0.0                 3.0   \n2032       0.031467       0.020000                 0.0                 1.0   \n2033       0.001000       0.025667                 2.0                 0.0   \n2034       0.001977       0.121617                 0.0                 1.0   \n2035       0.130400       0.040350                 4.0                 0.0   \n\n      LastMatchAwayWin  LastMatchHomeWin  ...  AwayLossRatio  AwayDrawRatio  \\\n0                    1                 0  ...       0.362887       0.271134   \n1                    0                 0  ...       0.385193       0.271459   \n2                    0                 1  ...       0.328947       0.289474   \n3                    1                 0  ...       0.365510       0.291757   \n4                    1                 0  ...       0.384536       0.276289   \n...                ...               ...  ...            ...            ...   \n2031                 0                 0  ...       0.390507       0.251348   \n2032                 0                 0  ...       0.324769       0.263104   \n2033                 0                 0  ...       0.267073       0.193902   \n2034                 0                 0  ...       0.337838       0.275184   \n2035                 0                 0  ...       0.338547       0.274860   \n\n      HomeTeamAvgShotsOnTarget  AwayTeamAvgShotsOnTarget  HomeTeamScoredRatio  \\\n0                     4.722680                  4.689437             0.484754   \n1                     4.822256                  4.246407             0.474050   \n2                     4.931718                  5.233534             0.504348   \n3                     5.389130                  5.093117             0.508261   \n4                     4.785567                  5.205083             0.492595   \n...                        ...                       ...                  ...   \n2031                  6.823096                  5.361601             0.668308   \n2032                  4.954779                  6.555630             0.532182   \n2033                  4.956103                  4.484721             0.478221   \n2034                  6.768564                  5.013760             0.639847   \n2035                  5.519249                  6.962318             0.498918   \n\n      AwayTeamScoredRatio  Bookie_Prediction_A  Bookie_Prediction_D  \\\n0                0.478541                 True                False   \n1                0.462791                False                False   \n2                0.490965                False                False   \n3                0.515563                False                False   \n4                0.503591                 True                False   \n...                   ...                  ...                  ...   \n2031             0.487054                 True                False   \n2032             0.635985                 True                False   \n2033             0.512730                False                False   \n2034             0.516239                False                False   \n2035             0.653339                 True                False   \n\n      Bookie_Prediction_H  final_prediction  \n0                   False                 0  \n1                    True                -1  \n2                    True                 0  \n3                    True                 1  \n4                   False                 0  \n...                   ...               ...  \n2031                False                 0  \n2032                False                 0  \n2033                 True                 1  \n2034                 True                 1  \n2035                False                 0  \n\n[2036 rows x 959 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Avg_away_odds</th>\n      <th>Avg_home_odds</th>\n      <th>Avg_draw_odds</th>\n      <th>Var_away_odds</th>\n      <th>Var_home_odds</th>\n      <th>Var_draw_odds</th>\n      <th>LastMatchAwayGoals</th>\n      <th>LastMatchHomeGoals</th>\n      <th>LastMatchAwayWin</th>\n      <th>LastMatchHomeWin</th>\n      <th>...</th>\n      <th>AwayLossRatio</th>\n      <th>AwayDrawRatio</th>\n      <th>HomeTeamAvgShotsOnTarget</th>\n      <th>AwayTeamAvgShotsOnTarget</th>\n      <th>HomeTeamScoredRatio</th>\n      <th>AwayTeamScoredRatio</th>\n      <th>Bookie_Prediction_A</th>\n      <th>Bookie_Prediction_D</th>\n      <th>Bookie_Prediction_H</th>\n      <th>final_prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.505000</td>\n      <td>2.908333</td>\n      <td>3.201667</td>\n      <td>0.000150</td>\n      <td>0.007777</td>\n      <td>0.007217</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.362887</td>\n      <td>0.271134</td>\n      <td>4.722680</td>\n      <td>4.689437</td>\n      <td>0.484754</td>\n      <td>0.478541</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.928333</td>\n      <td>2.413333</td>\n      <td>3.210000</td>\n      <td>0.015217</td>\n      <td>0.002187</td>\n      <td>0.003600</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.385193</td>\n      <td>0.271459</td>\n      <td>4.822256</td>\n      <td>4.246407</td>\n      <td>0.474050</td>\n      <td>0.462791</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.271667</td>\n      <td>2.228333</td>\n      <td>3.356667</td>\n      <td>0.007417</td>\n      <td>0.001017</td>\n      <td>0.004267</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.328947</td>\n      <td>0.289474</td>\n      <td>4.931718</td>\n      <td>5.233534</td>\n      <td>0.504348</td>\n      <td>0.490965</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.990000</td>\n      <td>2.456667</td>\n      <td>3.203333</td>\n      <td>0.004000</td>\n      <td>0.003067</td>\n      <td>0.004067</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.365510</td>\n      <td>0.291757</td>\n      <td>5.389130</td>\n      <td>5.093117</td>\n      <td>0.508261</td>\n      <td>0.515563</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.601667</td>\n      <td>2.631667</td>\n      <td>3.435000</td>\n      <td>0.002457</td>\n      <td>0.000617</td>\n      <td>0.008150</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.384536</td>\n      <td>0.276289</td>\n      <td>4.785567</td>\n      <td>5.205083</td>\n      <td>0.492595</td>\n      <td>0.503591</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2031</th>\n      <td>2.343333</td>\n      <td>2.836667</td>\n      <td>3.523333</td>\n      <td>0.011307</td>\n      <td>0.003787</td>\n      <td>0.012867</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.390507</td>\n      <td>0.251348</td>\n      <td>6.823096</td>\n      <td>5.361601</td>\n      <td>0.668308</td>\n      <td>0.487054</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2032</th>\n      <td>1.836667</td>\n      <td>3.806667</td>\n      <td>3.900000</td>\n      <td>0.002547</td>\n      <td>0.031467</td>\n      <td>0.020000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.324769</td>\n      <td>0.263104</td>\n      <td>4.954779</td>\n      <td>6.555630</td>\n      <td>0.532182</td>\n      <td>0.635985</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2033</th>\n      <td>3.350000</td>\n      <td>2.000000</td>\n      <td>3.746667</td>\n      <td>0.038000</td>\n      <td>0.001000</td>\n      <td>0.025667</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.267073</td>\n      <td>0.193902</td>\n      <td>4.956103</td>\n      <td>4.484721</td>\n      <td>0.478221</td>\n      <td>0.512730</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2034</th>\n      <td>7.693333</td>\n      <td>1.361667</td>\n      <td>5.101667</td>\n      <td>0.135267</td>\n      <td>0.001977</td>\n      <td>0.121617</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.337838</td>\n      <td>0.275184</td>\n      <td>6.768564</td>\n      <td>5.013760</td>\n      <td>0.639847</td>\n      <td>0.516239</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2035</th>\n      <td>1.411667</td>\n      <td>6.530000</td>\n      <td>4.965000</td>\n      <td>0.001897</td>\n      <td>0.130400</td>\n      <td>0.040350</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.338547</td>\n      <td>0.274860</td>\n      <td>5.519249</td>\n      <td>6.962318</td>\n      <td>0.498918</td>\n      <td>0.653339</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2036 rows × 959 columns</p>\n</div>"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predict[\"england\"][final_predict[\"england\"][\"\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:10:05.215431200Z",
     "start_time": "2024-01-07T18:10:05.171030400Z"
    }
   },
   "id": "c8c28b157401789"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5714e184674fcc0c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
